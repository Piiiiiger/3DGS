# 示例

# 第一周

## 第一次记录

3dgs类代码
class GaussianModel:

    def setup_functions(self):
        def build_covariance_from_scaling_rotation(scaling, scaling_modifier, rotation):
            L = build_scaling_rotation(scaling_modifier * scaling, rotation)
            actual_covariance = L @ L.transpose(1, 2)
            symm = strip_symmetric(actual_covariance)
            return symm
        
        self.scaling_activation = torch.exp
        self.scaling_inverse_activation = torch.log

        self.covariance_activation = build_covariance_from_scaling_rotation

        self.opacity_activation = torch.sigmoid
        self.inverse_opacity_activation = inverse_sigmoid

        self.rotation_activation = torch.nn.functional.normalize


    def __init__(self, sh_degree, optimizer_type="default"):
        self.active_sh_degree = 0
        self.optimizer_type = optimizer_type
        self.max_sh_degree = sh_degree  
        self._xyz = torch.empty(0)
        self._features_dc = torch.empty(0)
        self._features_rest = torch.empty(0)
        self._scaling = torch.empty(0)
        self._rotation = torch.empty(0)
        self._opacity = torch.empty(0)
        self.max_radii2D = torch.empty(0)
        self.xyz_gradient_accum = torch.empty(0)
        self.denom = torch.empty(0)
        self.optimizer = None
        self.percent_dense = 0
        self.spatial_lr_scale = 0
        self.setup_functions()

    def capture(self):
        return (
            self.active_sh_degree,
            self._xyz,
            self._features_dc,
            self._features_rest,
            self._scaling,
            self._rotation,
            self._opacity,
            self.max_radii2D,
            self.xyz_gradient_accum,
            self.denom,
            self.optimizer.state_dict(),
            self.spatial_lr_scale,
        )

    def restore(self, model_args, training_args):
        (self.active_sh_degree, 
        self._xyz, 
        self._features_dc, 
        self._features_rest,
        self._scaling, 
        self._rotation, 
        self._opacity,
        self.max_radii2D, 
        xyz_gradient_accum, 
        denom,
        opt_dict, 
        self.spatial_lr_scale) = model_args
        self.training_setup(training_args)
        self.xyz_gradient_accum = xyz_gradient_accum
        self.denom = denom
        self.optimizer.load_state_dict(opt_dict)

def build_covariance_from_scaling_rotation(scaling, scaling_modifier, rotation):构建协方差矩阵，先构建缩放矩阵L，再actual_covariance = L @ L.transpose(1, 2)计算协方差，分别传入的是缩放，缩放修正和旋转

接着就是指数函数用exp（x）为e的x次幂，保证缩放因子为正。

self.opacity_activation = torch.sigmoid，透明度[0,1]

上面的每一个运算都有逆运算

self.rotation_activation = torch.nn.functional.normalize,归一化函数，保持旋转四元数为单位四元数


下面是--init--
self.active_sh_degree = 0和self.max_sh_degree = sh_degree是球谐函数的最大阶和当前阶

self._xyz = torch.empty(0)，高斯的位置，用torch当前0阶作为参数，实际情况会变成传入的【N,3】矩阵，下面同样

self._features_dc = torch.empty(0) 球谐函数DC分量，表示基础颜色，【N,3】一般为（R，G,B）

self._features_rest = torch.empty(0) 球谐函数高频分量，根据最高阶来描述视角依赖的颜色变化为【N,(max_sh*max_sh -1)*3】，捕捉镜面反射、光泽效果，例如一阶的话是线性变化，二阶是二次变化

self._scaling = torch.empty(0)缩放参数[N，3]，控制高斯椭球在各轴的大小，决定协方差矩阵的特征值，形成缩放矩阵

self._rotation = torch.empty(0)旋转四元数[N, 4]，控制高斯椭球的方向，并且决定协方差矩阵的特征向量，高斯方向如果对着相机就是高贡献，背对即为低贡献

self._opacity = torch.empty(0)透明度参数[N,1]控制高斯点的可见性

self.max_radii2D = torch.empty(0)，最大2d投影半径，表示高斯点在图像平面上的覆盖范围，可以在渲染前用于剔除对当前视图贡献小的高斯点

self.xyz_gradient_accum = torch.empty(0) 位置梯度累积，[N, 3]对每个高斯点的xyz梯度累积

self.denom = torch.empty(0) 自适应过程中的分母项，每个高斯点一个值

self.optimizer = None 优化器

self.percent_dense = 0密集度百分比可以控制高斯点分裂或者克隆的强度，为【0，1】一般为0.01，前x%的高斯点，x为precent——dense,在自适应过程中的检查

self.spatial_lr_scale = 0 空间学习率缩放因子 根据高斯点对角线在背景的尺寸比例，消除场景尺寸对学习率的影响，防止大场景中小学习率导致收敛慢，小场景中大学习率导致震荡，学习率控制着模型在训练过程中更新权重的步长大小

def capture(self):
    return (
        self.active_sh_degree,
        self._xyz,
        self._features_dc,
        self._features_rest,
        self._scaling,
        self._rotation,
        self._opacity,
        self.max_radii2D,
        self.xyz_gradient_accum,
        self.denom,
        self.optimizer.state_dict(),  # 优化器状态
        self.spatial_lr_scale,
    )返回元组的状态，即作为一个接口获取传入元组的所有参数，用于保存检查点

def restore(self, model_args, training_args):
    (self.active_sh_degree, 
     self._xyz, 
     self._features_dc, 
     self._features_rest,
     self._scaling, 
     self._rotation, 
     self._opacity,
     self.max_radii2D, 
     xyz_gradient_accum, 
     denom,
     opt_dict, 
     self.spatial_lr_scale) = model_args
     
    self.training_setup(training_args)
    
    # 恢复优化状态
    self.xyz_gradient_accum = xyz_gradient_accum
    self.denom = denom
    self.optimizer.load_state_dict(opt_dict)
    ）实现训练中断后继续训练，先加载保存的模型状态，调用 training_setup 重新初始化优化器，然后恢复梯度累积等优化状态变量

## 第二次记录

下面是@property 装饰器定义，封装内部状态，提供一致接口，并且支持自动微分
    @property
    def get_scaling(self):获取激活后的缩放参数
        return self.scaling_activation(self._scaling)激活原始的参数
    
    @property
    def get_rotation(self):获取归一化后的旋转四元数
        return self.rotation_activation(self._rotation)
    
    @property
    def get_xyz(self):直接获取位置坐标
        return self._xyz
    
    @property
    def get_features(self):获取所有球谐系数
        features_dc = self._features_dc
        features_rest = self._features_rest
        return torch.cat((features_dc, features_rest), dim=1)用cat动态拼接DC和高阶分量
    
    @property
    def get_features_dc(self):获取球谐DC分量
        return self._features_dc
    
    @property
    def get_features_rest(self):获取球谐高阶分量
        return self._features_rest
    
    @property
    def get_opacity(self):获取激活后的不透明度
        return self.opacity_activation(self._opacity)
    
    @property
    def get_exposure(self):获取相机曝光参数，形状[N_cam, 3, 4]
        return self._exposure

def get_exposure_from_name(self, image_name):##曝光处理，获取特定图像的曝光参数

    if self.pretrained_exposures is None:##未训练
    
        return self._exposure[self.exposure_mapping[image_name]]#图像名称到参数索引的映射字典
    else:#预训练
    
        return self.pretrained_exposures[image_name]


def get_covariance(self, scaling_modifier = 1):##计算协方差矩阵

        return self.covariance_activation(self.get_scaling, scaling_modifier, self._rotation)


def oneupSHdegree(self):##提升球谐函数阶数，渐进式学习，先颜色，再反射，接着精细的反射

    if self.active_sh_degree < self.max_sh_degree:
    
        self.active_sh_degree += 1


def create_from_pcd(self, pcd : BasicPointCloud, cam_infos : int, spatial_lr_scale : float):点云初始化3D高斯模型
        self.spatial_lr_scale = spatial_lr_scale空间缩放
        fused_point_cloud = torch.tensor(np.asarray(pcd.points)).float().cuda()位置初始化，将点云位置转换为CUDA张量，np.asarray：转换为NumPy数组，torch.tensor：创建PyTorch张量，float()：转换为浮点类型，cuda()：移至GPU
        
        fused_color = RGB2SH(torch.tensor(np.asarray(pcd.colors)).float().cuda())将RGB转为球谐DC分量
        
        features = torch.zeros((fused_color.shape[0], 3, (self.max_sh_degree + 1) ** 2)).float().cuda()创建全零球谐系数张量
        
        features[:, :3, 0 ] = fused_color 将球谐DC分量放入第一个系数槽
        features[:, 3:, 1:] = 0.0 初始化所有高阶球谐系数为0

        print("Number of points at initialisation : ", fused_point_cloud.shape[0])

        dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)基于点间距计算初始缩放，在GPU上并行计算点间距，返回每个点的最小距离平方
        
        scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3) 缩放参数初始化，torch.log()：取对数 → 优化空间，[...,None]：增加维度 [N,1]，repeat(1,3)：复制到三个轴 [N,3]
        
        rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")初始化旋转四元数
        rots[:, 0] = 1 [1,0,0,0]

        opacities = self.inverse_opacity_activation(0.1 * torch.ones((fused_point_cloud.shape[0], 1), dtype=torch.float, device="cuda"))初始化透明度参数，创建全0.1张量 [N,1]，应用逆sigmoid

        self._xyz = nn.Parameter(fused_point_cloud.requires_grad_(True))将位置设为可训练参数，nn.Parameter 标记为模型参数，requires_grad_(True) 启用梯度
        
        self._features_dc = nn.Parameter(features[:,:,0:1].transpose(1, 2).contiguous().requires_grad_(True))注册球谐DC分量参数，切片获取DC分量 [N,3,1]，transpose(1,2) → [N,1,3]，contiguous() 确保内存连续
        
        self._features_rest = nn.Parameter(features[:,:,1:].transpose(1, 2).contiguous().requires_grad_(True))注册球谐高阶分量参数，[:,:,1:] 获取1阶及以上系数
        
        self._scaling = nn.Parameter(scales.requires_grad_(True))注册缩放参数，存储缩放后的对数值
        
        self._rotation = nn.Parameter(rots.requires_grad_(True))注册旋转四元数参数
        
        self._opacity = nn.Parameter(opacities.requires_grad_(True))注册透明度参数，存储逆simoid的值
        
        self.max_radii2D = torch.zeros((self.get_xyz.shape[0]), device="cuda")初始化2D投影半径为0
        
        self.exposure_mapping = {cam_info.image_name: idx for idx, cam_info in enumerate(cam_infos)}创建图像名到曝光参数索引的映射
        self.pretrained_exposures = None 标记无预训练曝光参数
        
        exposure = torch.eye(3, 4, device="cuda")[None].repeat(len(cam_infos), 1, 1)初始化曝光参数矩阵[N, 3, 4]
        
        self._exposure = nn.Parameter(exposure.requires_grad_(True))注册可学习的曝光参数


def training_setup(self, training_args):
        self.percent_dense = training_args.percent_dense设置密集度百分比，从训练参数获取密集度百分比值
        
        self.xyz_gradient_accum = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")创建并归零位置梯度累积张量，记录每个高斯点位置梯度的累积量
        
        self.denom = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")创建并归零分母项张量，记录每个高斯点的更新次数

        l = [
            {'params': [self._xyz], 'lr': training_args.position_lr_init * self.spatial_lr_scale, "name": "xyz"},
            {'params': [self._features_dc], 'lr': training_args.feature_lr, "name": "f_dc"},
            {'params': [self._features_rest], 'lr': training_args.feature_lr / 20.0, "name": "f_rest"},
            {'params': [self._opacity], 'lr': training_args.opacity_lr, "name": "opacity"},
            {'params': [self._scaling], 'lr': training_args.scaling_lr, "name": "scaling"},
            {'params': [self._rotation], 'lr': training_args.rotation_lr, "name": "rotation"}
        ]定义优化参数组，应用空间缩放因子，同时SH高频分量20倍降速降低学习率防止过拟合

        if self.optimizer_type == "default":
            self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)default (Adam），标准Adam优化器，通用场景
        elif self.optimizer_type == "sparse_adam":为稀疏梯度定制，大型场景(>100万高斯点)
            try:
                self.optimizer = SparseGaussianAdam(l, lr=0.0, eps=1e-15)
            except:
                # A special version of the rasterizer is required to enable sparse adam
                self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)

        self.exposure_optimizer = torch.optim.Adam([self._exposure])为曝光参数单独创建优化器，避免主优化器的干扰

        self.xyz_scheduler_args = get_expon_lr_func(lr_init=training_args.position_lr_init*self.spatial_lr_scale,
                                                    lr_final=training_args.position_lr_final*self.spatial_lr_scale,
                                                    lr_delay_mult=training_args.position_lr_delay_mult,
                                                    max_steps=training_args.position_lr_max_steps)位置学习率调度配置，初始学习率，最终学习率，延迟衰减因子，最大步数
        
        self.exposure_scheduler_args = get_expon_lr_func(training_args.exposure_lr_init, training_args.exposure_lr_final,
                                                        lr_delay_steps=training_args.exposure_lr_delay_steps,独立设置，曝光可能需要更早调整
                                                        lr_delay_mult=training_args.exposure_lr_delay_mult,
                                            全程调整曝光 max_steps=training_args.iterations)曝光学习率调度配置
