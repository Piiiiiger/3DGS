## 第二周的学习
## 第一次学习记录
<font color=royalblue>gaussian_render/init.py  将 3D 高斯点投影到 2D 图像平面，并进行混合以生成最终的图像。 </font>:
import torch
import math
from diff_gaussian_rasterization import GaussianRasterizationSettings, GaussianRasterizer
导入核心的 CUDA 加速栅格化模块

from scene.gaussian_model import GaussianModel
from utils.sh_utils import eval_sh  导入用于评估球谐函数,eval_sh能根据视角方向和 SH 系数计算出当前视角的颜色。

def render(viewpoint_camera, pc : GaussianModel, pipe, bg_color : torch.Tensor, scaling_modifier = 1.0, separate_sh = False, override_color = None, use_trained_exp=False):
    """
    Render the scene. 
    
    Background tensor (bg_color) must be on GPU!
    """
 
    # Create zero tensor. We will use it to make pytorch return gradients of the 2D (screen-space) means
    screenspace_points = torch.zeros_like(pc.get_xyz, dtype=pc.get_xyz.dtype, requires_grad=True, device="cuda") + 0
    创建一个与 3D 高斯点位置 pc.get_xyz 形状相同的零张量，+ 0 是为了创建一个新的张量副本，而不是直接修改 pc.get_xyz。


    try:
        screenspace_points.retain_grad()
        retain_grad() 确保即使在 backward() 之后，screenspace_points 的梯度仍然可用。

    except:
        pass

    # Set up rasterization configuration
    tanfovx = math.tan(viewpoint_camera.FoVx * 0.5)

    tanfovy = math.tan(viewpoint_camera.FoVy * 0.5)
    将相机的水平和垂直视场角（Field of View, FoV）转换为其一半的反正切值。这是栅格化器所需的参数，用于确定投影的几何关系。tan(FoV/2) = (传感器尺寸/2) / 像距，其中传感器尺寸和像距单位需一致‌


    raster_settings = GaussianRasterizationSettings(
        image_height=int(viewpoint_camera.image_height),
        image_width=int(viewpoint_camera.image_width),
        tanfovx=tanfovx,
        tanfovy=tanfovy,
        视场角的正切值，决定了投影的透视效果。

        bg=bg_color,用于填充没有高斯点覆盖的区域。

        scale_modifier=scaling_modifier,高斯点尺度的全局调整系数。

        viewmatrix=viewpoint_camera.world_view_transform,世界坐标系到相机坐标系的变换矩阵

        projmatrix=viewpoint_camera.full_proj_transform,投影矩阵，将相机坐标系下的 3D 点投影到 2D 屏幕空间

        sh_degree=pc.active_sh_degree,

        campos=viewpoint_camera.camera_center,相机在世界坐标系中的位置，用于计算视角方向

        prefiltered=False,
        debug=pipe.debug,
        antialiasing=pipe.antialiasing抗锯齿开关。
    )
    实例化 GaussianRasterizationSettings 对象，传入所有渲染所需的相机参数和管线配置，

    rasterizer = GaussianRasterizer(raster_settings=raster_settings)实例化 GaussianRasterizer 对象，传入配置好的 raster_settings


    means3D = pc.get_xyz
    means2D = screenspace_points
    opacity = pc.get_opacity
    从 GaussianModel (pc) 中获取 3D 位置 (means3D)、2D 屏幕空间位置（用于梯度计算）和不透明度

    # If precomputed 3d covariance is provided, use it. If not, then it will be computed from
    # scaling / rotation by the rasterizer.
    scales = None
    rotations = None
    cov3D_precomp = None

    if pipe.compute_cov3D_python:如果在 Python 中计算 3D 协方差，则调用pc.get_covariance

        cov3D_precomp = pc.get_covariance(scaling_modifier)
    else:否则，栅格化器会在 CUDA 内部根据尺度和旋转计算 3D 协方差
        scales = pc.get_scaling
        rotations = pc.get_rotation

    # If precomputed colors are provided, use them. Otherwise, if it is desired to precompute colors
    # from SHs in Python, do it. If not, then SH -> RGB conversion will be done by rasterizer.
     高斯点的颜色可以由球谐函数（SH）计算得到，也可以直接提供预计算的 RGB 颜色
    shs = None
    colors_precomp = None用来存储预计算的 RGB 颜色
    if override_color is None: 如果没有指定覆盖颜色

        if pipe.convert_SHs_python:在没有 override_color 的情况下，进一步检查渲染管线参数 pipe 中是否设置了 convert_SHs_python ，决定了 SH 到 RGB 的转换是在 Python 代码中进行，还是在底层的 CUDA 栅格化器内部进行

            shs_view = pc.get_features.transpose(1, 2).view(-1, 3, (pc.max_sh_degree+1)**2)先获取斯模型 pc 中存储的SH 系数，再交换张量的第 1 维和第 2 维，然后重塑张量，-1 表示 PyTorch 会自动计算该维度的大小，3 表示 RGB 三个颜色通道


            dir_pp = (pc.get_xyz - viewpoint_camera.camera_center.repeat(pc.get_features.shape[0], 1))先获取所有高斯点的 3D 坐标，再获取当前相机在世界坐标系中的 3D 坐标，将相机中心坐标重复pc.get_features.shape[0] 次（即高斯点的数量），使其与 pc.get_xyz 的维度匹配，便于逐点相减，得到从每个高斯点指向相机中心的向量

            dir_pp_normalized = dir_pp/dir_pp.norm(dim=1, keepdim=True)先计算 dir_pp 中每个向量的 L2 范数（长度），dim=1 表示对行进行范数计算，keepdim=True 保持维度，将 dir_pp 中的每个向量进行归一化，使其长度为 1。

            sh2rgb = eval_sh(pc.active_sh_degree, shs_view, dir_pp_normalized)调用 eval_sh 函数根据相机方向评估 SH，得到当前视角的颜色

            colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)返回的 SH 颜色值通常在 -0.5 到 0.5 之间。将其加上 0.5，可以将其范围映射到 0 到 1 之间映射到 RGB 颜色范围（0 到 1）再将所有小于 0.0 的值裁剪为 0.0，计算出的 SH 颜色值从数学范围映射到有效的 RGB 颜色范围 [0, 1]，确保颜色是非负的，并且在栅格化时能够正确显示
        else:SH 到 RGB 的转换将在 CUDA 栅格化器内部进行

            if separate_sh:检查 SH 系数是否被分离存储

                dc, shs = pc.get_features_dc, pc.get_features_rest 如果分离存储，则获取 DC 分量和其余的 SH 分量。

            else:
                shs = pc.get_features  直接获取所有 SH 系数作为 shs

    else:
        colors_precomp = override_color  直接将传入的 override_color 赋值给 colors_precomp，如果外部已经指定了颜色，就直接使用它，而不再进行 SH 评估

    # Rasterize visible Gaussians to image, obtain their radii (on screen). 将可见的高斯点栅格化到图像上，并获取它们在屏幕上的半径高度，这是并行化的 GPU 操作，3D到2D投影: 将每3D高斯点（由 means3D、scales、rotations/cov3D_precomp 定义）投影到相机平面，形成一个2D 高斯，然后根据投影后的高斯点中心到相机的深度进行排序。为了正确的透明度混合，需要从后往前（或从前往后）处理高斯点，对于图像中的每个像素，遍历与其重叠的 2D 高斯椭圆。根据每个高斯点的不透明度 (opacities) 和颜色 (dc/shs/colors_precomp)，使用 alpha 混合公式将它们叠加起来，计算最终的像素颜色（体渲染公式）。

    if separate_sh:检查球谐函数（SH）的直流（DC）分量和高阶分量是否是分开存储和处理的
        rendered_image, radii, depth_image = rasterizer(
            means3D = means3D,高斯点的 3D 中心坐标

            means2D = means2D,斯点在屏幕空间中的 2D 投影中心。这个参数虽然在 CUDA 内部计算，但传入一个 requires_grad=True 的张量是为了捕获从 2D 屏幕空间到 3D 均值的梯度

            dc = dc,仅当 separate_sh 为 True 时提供

            shs = shs,球谐函数的高阶分量

            colors_precomp = colors_precomp,预计算的 RGB 颜色。如果 pipe.convert_SHs_python 为 True 或 override_color 存在，那么颜色会在这里直接提供，shs 将不被使用

            opacities = opacity,每个高斯点的不透明度。这决定了高斯点在混合时的权重

            scales = scales,高斯点的 3D 尺度向量。描述高斯椭球体的三个主轴长度

            rotations = rotations,旋转四元数

            cov3D_precomp = cov3D_precomp预计算的 3D 协方差矩阵。如果提供了这个参数，栅格化器会直接使用它，而不是从 scales 和 rotations 动态计算
            )
            将所有必要的高斯点属性和渲染配置传递给 rasterizer 对象

    else:SH 系数没有被分离存储，栅格化器会根据传入的 shs 参数（如果 colors_precomp 为 None）在内部完成 SH 到 RGB 的转换和颜色计算。

        rendered_image, radii, depth_image = rasterizer(
            means3D = means3D,
            means2D = means2D,
            shs = shs,
            colors_precomp = colors_precomp,
            opacities = opacity,
            scales = scales,
            rotations = rotations,
            cov3D_precomp = cov3D_precomp)
        
    # Apply exposure to rendered image (training only)
    对渲染图像应用曝光调整，并且通常只在训练阶段进行

    if use_trained_exp:
    如果 use_trained_exp 为 True，则执行曝光调整逻辑，而use_trained_exp 是一个布尔标志，控制是否应用训练好的曝光参数。在测试或最终渲染时，可能不需要应用这个动态曝光，或者会使用一个固定的曝光，用来根据训练/测试阶段或特定需求来控制曝光调整。

        exposure = pc.get_exposure_from_name(viewpoint_camera.image_name)
        从高斯模型 pc 中获取与当前相机视图对应的曝光参数。这个函数会根据 viewpoint_camera.image_name查找预先学习到的曝光值，检索与当前渲染视图关联的、由模型训练得到的曝光变换矩阵，曝光参数通常被建模为一个4x4的齐次变换矩阵，其中左上角的3x3子矩阵表示颜色空间的线性变换，最后一列的3x1向量表示亮度偏移

        rendered_image = torch.matmul(rendered_image.permute(1, 2, 0), exposure[:3, :3]).permute(2, 0, 1) + exposure[:3, 3,   None, None]
        先将将 rendered_image 的维度从 [C, H, W] (通道, 高度, 宽度) 变换到 [H, W, C]，让进行矩阵乘法时，将颜色通道作为最后一个维度，在获取获取 exposure 矩阵的左上角3x3部分，这是颜色空间的线性变换矩阵，再用torch.matmul执行矩阵乘法，渲染图像的每个像素的 RGB 向量 [R, G, B] 与3x3的颜色变换矩阵相乘，实现颜色空间变换，再将结果维度再次从 [H, W, C] 变换回 [C, H, W]，以恢复图像的常用格式再与亮度偏移相加 获取 exposure 矩阵的最后一列的3x1向量（表示平移/亮度偏移），并通过 None 扩展维度，使其形状变为 [3, 1, 1]，将学到的曝光变换（包括颜色空间调整和亮度偏移）应用到渲染图像上，使其更接近真实的图像条件。

    # Those Gaussians that were frustum culled or had a radius of 0 were not visible.
    # They will be excluded from value updates used in the splitting criteria.
    有些高斯点不可见：它们可能在相机的视锥体之外（被裁剪），或者它们的投影半径为零，它们将从用于分裂/克隆高斯点的统计数据中排除，不可见的高斯点对当前渲染结果没有贡献，因此它们的梯度为零。在自适应稠密化和剪枝过程中，只关心那些对图像有实际贡献的高斯点，以确保优化过程的效率和有效性。

    rendered_image = rendered_image.clamp(0, 1)将 rendered_image 的像素值限制在 [0, 1] 的范围内确保最终输出的图像具有有效的像素值，适合保存和显示

    out = {
        "render": rendered_image,最终渲染出的 RGB 图像

        "viewspace_points": screenspace_points,高斯点在屏幕空间中的 2D 投影点

        "visibility_filter" : (radii > 0).nonzero(),创建一个布尔张量，其中 True 表示对应的高斯点在屏幕上具有非零的投影半径（即可见），再返回布尔张量中所有 True 元素的索引

        "radii": radii,每个高斯点在屏幕空间中的投影半径。

        "depth" : depth_image渲染出的深度图
        
        }封装渲染函数的所有重要输出
    
    return out

## 第二次学习记录
<font color="royalblue">代码中的camera类，承载了渲染一张图像所需的所有几何信息和图像本身的数据，定义了“一个相机是如何看世界”以及“这张视角下的原始图像长什么样”，是 3DGS 渲染算法的直接操作对象</font>

import torch
from torch import nn
import numpy as np
from utils.graphics_utils import getWorld2View2, getProjectionMatrix
from utils.general_utils import PILtoTorch
import cv2

class Camera(nn.Module):
    def __init__(self, resolution, colmap_id, R, T, FoVx, FoVy, depth_params, image, invdepthmap,
                 image_name, uid,
                 trans=np.array([0.0, 0.0, 0.0]), scale=1.0, data_device = "cuda",
                 train_test_exp = False, is_test_dataset = False, is_test_view = False
                 ):
        R是旋转矩阵（3x3 NumPy 数组），表示相机在世界坐标系中的方向。它将点从世界坐标系旋转到相机坐标系。
        T是平移向量（3x1 NumPy 数组），表示相机在世界坐标系中的位置。它将点从世界坐标系平移到相机坐标系
        depth_params是一个字典，包含与深度图缩放和偏移相关的参数
        invdepthmap是一个 NumPy 数组，表示此相机视图的逆深度图。如果不可用，则为 None


        super(Camera, self).__init__()

        self.uid = uid
        self.colmap_id = colmap_id
        self.R = R
        self.T = T
        self.FoVx = FoVx
        self.FoVy = FoVy
        self.image_name = image_name

        try:
            self.data_device = torch.device(data_device)
        except Exception as e:
            print(e)
            print(f"[Warning] Custom device {data_device} failed, fallback to default cuda device" )
            self.data_device = torch.device("cuda")

        resized_image_rgb = PILtoTorch(image, resolution)将 PIL 图像转换为 PyTorch 张量，并可能将其大小调整为 resolution
        从 resized_image_rgb 张量中提取 RGB 通道（前 3 个通道）

        self.alpha_mask = None  

        
        if resized_image_rgb.shape[0] == 4:如果输入图像有 4 个通道 (RGBA)，则提取alpha通道，并将alpha掩码张量移动到指定的计算设

            self.alpha_mask = resized_image_rgb[3:4, ...].to(self.data_device)

        else: 如果图像只有 3 个通道 (RGB)，则创建一个全为 1 的掩码，将alpha掩码张量移动到指定的计算设。这意味着所有像素都被视为不透明或有效，

            self.alpha_mask = torch.ones_like(resized_image_rgb[0:1, ...].to(self.data_device))

        通过隐藏真实图像的一部分，可以测试模型合成新视图或填充缺失信息的能力
        if train_test_exp and is_test_view:如果 train_test_exp 为真且这是 is_test_view

            if is_test_dataset:如果is_test_dataset也为真，则图像 alpha 掩码的左半部分设置为 0，有效地使这些像素在渲染或损失计算期间透明
                self.alpha_mask[..., :self.alpha_mask.shape[-1] // 2] = 0
            else:如果 is_test_dataset 为假，意味着它是来自训练数据集的测试视图，则图像alpha掩码的右半部分设置为 0
                self.alpha_mask[..., self.alpha_mask.shape[-1] // 2:] = 0

        self.original_image = gt_image.clamp(0.0, 1.0).to(self.data_device)
        self.image_width = self.original_image.shape[2]
        self.image_height = self.original_image.shape[1]

        self.invdepthmap = None
        self.depth_reliable = False
        if invdepthmap is not None:
            self.depth_mask = torch.ones_like(self.alpha_mask)创建一个全为 1 的掩码，最初假设所有深度值都是可靠的
            self.invdepthmap = cv2.resize(invdepthmap, resolution)使用 OpenCV 将输入的逆深度图调整到目标分辨率
            self.invdepthmap[self.invdepthmap < 0] = 0将负的逆深度值钳制为零
            self.depth_reliable = True 深度信息可用且最初被认为是可靠的

            if depth_params is not None:
                if depth_params["scale"] < 0.2 * depth_params["med_scale"] or depth_params["scale"] > 5 * depth_params  ["med_scale"]:  检查scale参数，太大或太小将depth_reliable标志设置为False，并将depth_mask设置为全零，过滤掉可能错误或无用的深度图
                    self.depth_reliable = False
                    self.depth_mask *= 0
                
                if depth_params["scale"] > 0: 为正，则对invdepthmap进行缩放和偏移
                    self.invdepthmap = self.invdepthmap * depth_params["scale"] + depth_params["offset"]

            if self.invdepthmap.ndim != 2:确保逆深度图是 2D 的
                self.invdepthmap = self.invdepthmap[..., 0]
            self.invdepthmap = torch.from_numpy(self.invdepthmap[None]).to(self.data_device)将NumPy数组转换为 PyTorch 张量，并添加一个批次维度

        self.zfar = 100.0
        self.znear = 0.01
        定义了相机视锥体的近裁剪面和远裁剪面。比znear近或比zfar远的物体通常会被裁剪而不被渲染


        self.trans = trans
        self.scale = scale
        平移和缩放因子

        self.world_view_transform = torch.tensor(getWorld2View2(R, T, trans, scale)).transpose(0, 1).cuda()将一个4x4矩阵，一个将3D点从世界坐标系转换为相机（视图）坐标系的矩阵的逆矩阵变成张量

        self.projection_matrix = getProjectionMatrix(znear=self.znear, zfar=self.zfar, fovX=self.FoVx, fovY=self.FoVy).transpose(0,1).cuda()计算投影矩阵，将3D点从相机（视图）坐标系转换为2D齐次裁剪坐标，矩阵用来定义视锥体和映射。
        裁剪坐标中的点通常通过其“w”分量（w是齐次坐标的第四个维度，因为在笛卡尔坐标系中，平移是一个加法运算，而旋转和缩放是乘法运算。这使得它们难以用一个统一的矩阵乘法来表示，引入 w 分量，可以将 3D 点 (x,y,z) 表示为 4D 齐次坐标 (x',y' ,z' ,w' ),然后实际的 3D 笛卡尔坐标是 (x'/w' ,y'/w  ,z'/w'),变成4x4的矩阵乘法能在图形编程中简化变换链的处理，因为你需要对一个物体依次进行平移、旋转和缩放操作，你可以分别构建平移矩阵T、旋转矩阵 R和缩放矩阵S，得到一个最终的组合变换矩阵 M,M =T ⋅ R ⋅ S）
        进行除法（透视除法）以获得归一化设备坐标 (NDC)，然后缩放/平移到屏幕坐标

        self.full_proj_transform = (self.world_view_transform.unsqueeze(0).bmm(self.projection_matrix.unsqueeze(0))).squeeze(0)
        这是世界到视图变换和投影矩阵的串联，这个单一的4x4矩阵可以直接将3D世界点转换为2D齐次裁剪坐标

        self.camera_center = self.world_view_transform.inverse()[3, :3]世界坐标系中的相机中心可以通过世界到视图变换矩阵的逆矩阵找到。逆矩阵的最后一列（或行，取决于约定），特别是前三个元素，将表示相机在世界坐标系中的位置
        
class MiniCam:
    def __init__(self, width, height, fovy, fovx, znear, zfar, world_view_transform, full_proj_transform):
        self.image_width = width
        self.image_height = height    
        self.FoVy = fovy
        self.FoVx = fovx
        self.znear = znear
        self.zfar = zfar

        self.world_view_transform = world_view_transform 预计算的世界到视图矩阵

        self.full_proj_transform = full_proj_transform 预计算的完整投影矩阵

        view_inv = torch.inverse(self.world_view_transform)计算世界到视图矩阵的逆矩阵，它将点从相机坐标系转换到世界坐标系

        self.camera_center = view_inv[3][:3]从逆矩阵中提取相机在世界坐标系中的位置


## 第三次学习记录
书包被坏人偷了，伤心，今天休息

## 第四次学习记录
<font color="royalblue">损失函数定义，SSIM指数旨在从亮度、对比度和结构三个方面衡量两幅图像的相似性</font>
import torch
import torch.nn.functional as F
from torch.autograd import Variable
from math import exp
try:
    from diff_gaussian_rasterization._C import fusedssim, fusedssim_backward
except:
    pass

C1 = 0.01 ** 2
C2 = 0.03 ** 2
C1 和 C2 是SSIM计算中的稳定项,用于避免分母为零的情况
SSIM(x,y)=(2μx*μy+C1)(σxy +C2)/(μx*μx+μy*μy+C1)(σx*σx+σy*σy +C2)

​


class FusedSSIMMap(torch.autograd.Function):用于实现 fast_ssim,允许我们定义前向传播和反向传播的计算图
    @staticmethod
    def forward(ctx, C1, C2, img1, img2):前向传播函数

        ssim_map = fusedssim(C1, C2, img1, img2)fusedssim 来计算 SSIM 的映射图

        ctx.save_for_backward(img1.detach(), img2)将img1(分离梯度)和img2保存到ctx,ctx是一个上下文对象，用于在反向传播时检索这些保存的张量

        ctx.C1 = C1
        ctx.C2 = C2
        return ssim_map

    @staticmethod
    def backward(ctx, opt_grad):反向传播函数，用于计算输入张量的梯度
        img1, img2 = ctx.saved_tensors 从ctx中获取在forward方法中保存的img1和img2

        C1, C2 = ctx.C1, ctx.C2 从ctx中获取保存的C1和C2

        grad = fusedssim_backward(C1, C2, img1, img2, opt_grad)根据SSIM公式的链式法则计算img1和img2的梯度
        return None, None, grad, None

def l1_loss(network_output, gt):损失函数L1
    return torch.abs((network_output - gt)).mean()计算网络输出图像和真实图像之间的像素级差异的绝对值再取平均
    L1=∑N|（yi-预测yi）|/N

def l2_loss(network_output, gt):L2 损失
    return ((network_output - gt) ** 2).mean()计算网络输出图像和真实图像之间的像素级差异再取平方，再取平均

def gaussian(window_size, sigma):生成一个一维高斯核，高斯形式e的−frac(x−mu)的平方乘2sigma的平方的积次幂
    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])遍历 window_size 范围内的所有点，计算每个点的高斯值，再对生成的高斯核进行归一化
    x是距离高斯核中心的距离。window_size // 2 是核的中心点，sigma是标准差
    return gauss / gauss.sum()

def create_window(window_size, channel):创建一个二维高斯卷积窗，用于SSIM计算中的局部平均

    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)调用gaussian函数生成一个一维高斯核，并使用unsqueeze(1)将其形状变为 (window_size, 1)，以便进行矩阵乘法。sigma固定为1.5

    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)通过两个一维高斯核的外积（矩阵乘法）生成一个二维高斯核，再将将形状从 (window_size, window_size) 变为 (1, 1, window_size, window_size)满足pytorch的conv2d函数所需的卷积核的形状

    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())将二维高斯核扩展到所有通道，如果图像有3个通道（RGB），那么这个高斯核会被复制3次，每个通道使用相同的核
    return window

def ssim(img1, img2, window_size=11, size_average=True):SSIM的入口函数

    channel = img1.size(-3)从末尾倒数第三个维度，获取输入图像的通道数

    window = create_window(window_size, channel)创建适用于所有通道的高斯卷积窗

    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)确保高斯窗的数据类型与输入图像一致

    return _ssim(img1, img2, window, window_size, channel, size_average)

def _ssim(img1, img2, window, window_size, channel, size_average=True):SSIM核心计算逻辑
    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)使用高斯窗对 img1 进行二维卷积，计算图像每个局部区域的加权平均亮度

    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)与上面操作一样

    计算局部均值的平方和和乘积
    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq 局部方差，对平方图像进行局部加权平均
    Var(X)=E[X*X]-E[x]*E[X]

    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq 局部方差
    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2协方差

    C1 = 0.01 ** 2
    C2 = 0.03 ** 2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))逐像素地应用SSIM公式，得到一张SSIM值图

    if size_average:
        return ssim_map.mean()返回整个SSIM映射图的平均值，得到一个单一的SSIM分数
    else:
        return ssim_map.mean(1).mean(1).mean(1)对每个通道的每个图像求平均，返回一个形状为 (batch_size,) 的张量，表示每个图像的SSIM值


def fast_ssim(img1, img2):使用自定义CUDA扩展实现的快速SSIM
    ssim_map = FusedSSIMMap.apply(C1, C2, img1, img2)调用之前定义的 FusedSSIMMap 类的apply方法。这会触发FusedSSIMMap的forward方法，进而调用底层的fusedssimCUDA函数，实现高性能的SSIM映射图计算
    return ssim_map.mean()



## 第五次学习记录
<font color="royalblue">3d高斯训练代码</font>

#
# Copyright (C) 2023, Inria
# GRAPHDECO research group, https://team.inria.fr/graphdeco
# All rights reserved.
#
# This software is free for non-commercial, research and evaluation use 
# under the terms of the LICENSE.md file.
#
# For inquiries contact  george.drettakis@inria.fr
#

import os
import torch
from random import randint
from utils.loss_utils import l1_loss, ssim
from gaussian_renderer import render, network_gui
import sys
from scene import Scene, GaussianModel
from utils.general_utils import safe_state, get_expon_lr_func
import uuid
from tqdm import tqdm
from utils.image_utils import psnr
from argparse import ArgumentParser, Namespace
from arguments import ModelParams, PipelineParams, OptimizationParams
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_FOUND = True
except ImportError:
    TENSORBOARD_FOUND = False

try:
    from fused_ssim import fused_ssim
    FUSED_SSIM_AVAILABLE = True
except:
    FUSED_SSIM_AVAILABLE = False

try:
    from diff_gaussian_rasterization import SparseGaussianAdam
    SPARSE_ADAM_AVAILABLE = True
except:
    SPARSE_ADAM_AVAILABLE = False

def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from):

    if not SPARSE_ADAM_AVAILABLE and opt.optimizer_type == "sparse_adam":如果用户在命令行中指定了使用 sparse_adam 优化器 (opt.optimizer_type == "sparse_adam")，但相关的库 diff_gaussian_rasterization 没有被成功导入

        sys.exit(f"Trying to use sparse adam but it is not installed, please install the correct rasterizer using pip install [3dgs_accel].")

    first_iter = 0  记录训练的起始迭代次数

    tb_writer = prepare_output_and_logger(dataset)该函数会根据命令行参数设置输出目录，并在该目录下创建日志文件（用于保存参数）和 TensorBoard 日志目录，设置输出目录并初始化 TensorBoard 写入器

    gaussians = GaussianModel(dataset.sh_degree, opt.optimizer_type)创建 3D 高斯模型实例

    scene = Scene(dataset, gaussians)创建场景管理实例，加载由 COLMAP 处理后的数据，读取 COLMAP 生成的稀疏点云，使用这些稀疏点云来初始化 gaussians 对象

    gaussians.training_setup(opt)
    if checkpoint:表示要从之前保存的检查点恢复训练
        (model_params, first_iter) = torch.load(checkpoint)加载检查点文件，它包含了保存的模型参数和当时的迭代次数

        gaussians.restore(model_params, opt)将加载的model_params应用到当前的实例中，使其恢复到之前的训练状态

    bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]确定渲染时使用的背景颜色。如果为True，则背景为白色，否则为黑色

    background = torch.tensor(bg_color, dtype=torch.float32, device="cuda")老朋友了

    iter_start = torch.cuda.Event(enable_timing = True)
    iter_end = torch.cuda.Event(enable_timing = True)
    在 CUDA 设备上记录时间戳，可以精确测量 CUDA 操作的执行时间

    use_sparse_adam = opt.optimizer_type == "sparse_adam" and SPARSE_ADAM_AVAILABLE 决定是否使用优化的稀疏 Adam 优化器

    depth_l1_weight = get_expon_lr_func(opt.depth_l1_weight_init, opt.depth_l1_weight_final, max_steps=opt.iterations)一个介于 opt.depth_l1_weight_init 和 opt.depth_l1_weight_final 之间的值，并按指数衰减。这个函数将作为深度损失的权重。其目的是在训练初期更强调深度一致性，而在后期逐渐减弱其影响。

    viewpoint_stack = scene.getTrainCameras().copy()获取所有训练相机的列表副本，这个列表会作为训练循环中选择相机的空间

    viewpoint_indices = list(range(len(viewpoint_stack)))创建一个与 viewpoint_stack 长度相同的索引列表。在训练循环中，我们会从这个索引列表中随机抽取，然后移除，确保每个相机在一定轮次内都被选中

    ema_loss_for_log = 0.0
    ema_Ll1depth_for_log = 0.0
    初始化两个变量，用于存储指数移动平均 (EMA) 损失。EMA是一种平滑数据点的方法，通过将当前值和历史值的加权平均来减少噪声，使得在进度条和日志中显示的损失曲线更加平滑，更易于观察趋势，而不是被每次迭代的瞬时波动所干扰

    progress_bar = tqdm(range(first_iter, opt.iterations), desc="Training progress")
    初始化tqdm进度条。range(first_iter, opt.iterations)定义了循环的范围，desc参数设置了进度条的描述文本

    first_iter += 1如果是从检查点恢复，first_iter已经被更新。这里加上1是为了让 range 函数从正确的迭代开始
    for iteration in range(first_iter, opt.iterations + 1):它从 first_iter 开始，一直运行到 opt.iterations
        if network_gui.conn == None:
            network_gui.try_connect()尝试连接 GUI 服务器
        while network_gui.conn != None:如果 GUI 连接成功，程序会进入这个 while 循环，暂停训练，等待来自 GUI 的命令
            try:
                net_image_bytes = None
                custom_cam, do_training, pipe.convert_SHs_python, pipe.compute_cov3D_python, keep_alive, scaling_modifer = network_gui.receive()接收来自 GUI 的数据，这可能包括一个自定义的相机姿态（用于用户在 GUI 中自由探索场景）、是否继续训练的指令 (do_training)、以及一些渲染管线相关的配置

                if custom_cam != None:如果 GUI 发送了一个自定义相机，那么程序会使用这个相机视角来调用 render 函数，渲染出当前高斯模型在该视角下的图像

                    net_image = render(custom_cam, gaussians, pipe, background, scaling_modifier=scaling_modifer, use_trained_exp=dataset.train_test_exp, separate_sh=SPARSE_ADAM_AVAILABLE)["render"]使用自定义相机渲染图像
                    
                    

                    net_image_bytes = memoryview((torch.clamp(net_image, min=0, max=1.0) * 255).byte().permute(1, 2, 0).contiguous().cpu().numpy())将渲染结果转换为字节流（JPEG）发送回 GUI

                network_gui.send(net_image_bytes, dataset.source_path) 将渲染出的图像（或其他信息）以字节流的形式发送回 GUI，使得用户可以在 GUI 中实时看到渲染效果

                if do_training and ((iteration < int(opt.iterations)) or not keep_alive):如果GUI发出信号 (do_training为True) 并且训练尚未结束，或者GUI不要求持续连接，那么程序会break跳出while循环，继续执行训练代码

                    break
            except Exception as e:
                network_gui.conn = None

        iter_start.record()在每个训练迭代的实际计算开始之前，记录一个 CUDA 事件，用于后续计算本迭代的 GPU 耗时

        gaussians.update_learning_rate(iteration)根据当前训练的iteration值，动态地调整所有高斯参数

        # Every 1000 its we increase the levels of SH up to a maximum degree
        if iteration % 1000 == 0:每 1000 次训练迭代执行一次。
            gaussians.oneupSHdegree()增加用于表示高斯颜色和光照的球谐函数 (SH) 的阶数,在训练初期，使用较低阶的 SH，可以更快地学习到场景大致的颜色和光照。随着训练的进行，逐渐增加 SH 阶数，允许模型学习更丰富的细节和更真实的光照效果，从而提高渲染质量。

        # Pick a random Camera
        if not viewpoint_stack:当所有相机都被抽取完,它会重新从 scene.getTrainCameras() 中获取一份新的相机列表，开始新一轮的随机抽取
            viewpoint_stack = scene.getTrainCameras().copy()
            viewpoint_indices = list(range(len(viewpoint_stack)))重新生成对应的索引列表
        rand_idx = randint(0, len(viewpoint_indices) - 1)从当前的相机索引列表中随机选择一个索引
        viewpoint_cam = viewpoint_stack.pop(rand_idx)从相机栈中移除并获取该索引对应的相机对象
        vind = viewpoint_indices.pop(rand_idx)从索引列表中移除该索引


        # Render
        if (iteration - 1) == debug_from:
            pipe.debug = True如果当前迭代是调试开始迭代，则开启渲染管道的调试模式,用于在特定的迭代输出更多中间信息或开启可视化调试功能

        bg = torch.rand((3), device="cuda") if opt.random_background else background  如果opt.random_background为 True，每次渲染时背景颜色都会是随机生成的 (3 个 0-1 之间的浮点数)。这可以帮助模型更好地学习场景中的物体，而不是将背景颜色编码到高斯中。否则使用初始化时设置的固定背景颜色 (background 变量，白色或黑色)

        render_pkg = render(viewpoint_cam, gaussians, pipe, bg, use_trained_exp=dataset.train_test_exp, separate_sh=SPARSE_ADAM_AVAILABLE)调用了从 gaussian_renderer 导入的 render 函数，即我这周最开始看的，返回一个字典 render_pkg

        image, viewspace_point_tensor, visibility_filter, radii = render_pkg["render"], render_pkg["viewspace_points"], render_pkg["visibility_filter"], render_pkg["radii"]

        if viewpoint_cam.alpha_mask is not None:如果当前相机视图带有 alpha_mask（通常用于指示图像中感兴趣的区域，例如在背景移除后），那么这个掩码会被加载到 GPU，并逐元素地乘到渲染图像 image 上。这使得渲染结果在掩码区域外变为黑色（或透明，如果后续处理透明度）。这在处理带有透明背景或需要关注特定前景物体的场景时非常有用。

            alpha_mask = viewpoint_cam.alpha_mask.cuda()
            image *= alpha_mask

        # Loss
        gt_image = viewpoint_cam.original_image.cuda()获取当前训练视角对应的真实图像，并将其传输到 GPU。

        Ll1 = l1_loss(image, gt_image)计算 L1 损失

        if FUSED_SSIM_AVAILABLE:

            ssim_value = fused_ssim(image.unsqueeze(0), gt_image.unsqueeze(0)) 计算 SSIM 

        else:
            ssim_value = ssim(image, gt_image)

        loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim_value)
        这是最终的总损失函数。它结合了L1损失和SSIM损失。opt.lambda_dssim是一个权重参数，控制 SSIM损失在总损失中的贡献。1.0-ssim_value是因为 SSIM值越大表示越相似（损失应该越小），所以我们用1-SSIM来将其转换为一个损失项（越小越好）。这种组合损失有助于在保留细节的同时保持图像的整体结构和感知质量。

        # Depth regularization
        Ll1depth_pure = 0.0

        if depth_l1_weight(iteration) > 0 and viewpoint_cam.depth_reliable:只有当深度损失的权重 大于 0 并且当前相机提供了可靠的深度信息 时，才会计算并应用深度损失

            invDepth = render_pkg["depth"]从渲染结果中获取高斯模型渲染出的逆深度图，逆深度比正深度在优化时更稳定，因为它可以处理无限远处的点

            mono_invdepth = viewpoint_cam.invdepthmap.cuda()如果数据集提供了从单目深度估计方法（如 MiDaS）预计算得到的逆深度图，它会被加载并作为深度真值。

            depth_mask = viewpoint_cam.depth_mask.cuda()一个掩码，指示图像中哪些区域的深度估计是可靠的

            Ll1depth_pure = torch.abs((invDepth  - mono_invdepth) * depth_mask).mean()计算渲染出的逆深度图与单目估计逆深度图之间在 depth_mask 区域的 L1 损失

            Ll1depth = depth_l1_weight(iteration) * Ll1depth_pure  将这个纯深度 L1 损失乘以一个随迭代次数动态衰减的权重。这表示在训练早期，深度损失可能更重要，以帮助快速收敛到合理的几何结构；而在后期，图像内容损失（L1+SSIM）会占据主导

            loss += Ll1depth 将深度损失加到总损失中
            Ll1depth = Ll1depth.item()
        else:
            Ll1depth = 0

        loss.backward()反向传播，计算所有可训练参数的梯度

        iter_end.record()记录当前迭代结束时间

        with torch.no_grad():在此块内执行的操作不计算梯度，不影响反向传播
            # Progress bar
            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log 更新 EMA 损失

            ema_Ll1depth_for_log = 0.4 * Ll1depth + 0.6 * ema_Ll1depth_for_log更新 EMA 深度损失

            if iteration % 10 == 0:
                progress_bar.set_postfix({"Loss": f"{ema_loss_for_log:.{7}f}", "Depth Loss": f"{ema_Ll1depth_for_log:.{7}f}"})更新进度条显示信息

                progress_bar.update(10) 进度条前进10步
            if iteration == opt.iterations:
                progress_bar.close()训练结束，关闭进度条


            # Log and save
            training_report(tb_writer, iteration, Ll1, loss, l1_loss, iter_start.elapsed_time(iter_end), testing_iterations, scene, render, (pipe, background, 1., SPARSE_ADAM_AVAILABLE, None, dataset.train_test_exp), dataset.train_test_exp)
            调用辅助函数，根据 testing_iterations 在特定迭代进行测试评估，并将训练指标记录到 TensorBoard。
            
            if (iteration in saving_iterations):
                print("\n[ITER {}] Saving Gaussians".format(iteration))
                scene.save(iteration)在 saving_iterations 指定的迭代保存当前的高斯模型状态。

            # Densification
            if iteration < opt.densify_until_iter:只有在达到某个迭代之前才进行增密操作

                # Keep track of max radii in image-space for pruning
                gaussians.max_radii2D[visibility_filter] = torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter])跟踪每个高斯在图像平面上所占的最大像素区域

                gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)统计可见高斯的梯度信息，用于增密和剪枝


                if iteration > opt.densify_from_iter and iteration % opt.densification_interval == 0:在特定迭代范围和间隔进行增密和剪枝

                    size_threshold = 20 if iteration > opt.opacity_reset_interval else None  大小阈值，用于剪枝过大的高
                    gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, scene.cameras_extent, size_threshold, radii)增密和剪枝

                
                if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter):
                    gaussians.reset_opacity()定期重置高斯的不透明度

            # Optimizer step
            if iteration < opt.iterations:在最后一迭代不执行优化器步进

                gaussians.exposure_optimizer.step()如果模型有独立的曝光优化器，则执行其步进。
                gaussians.exposure_optimizer.zero_grad(set_to_none = True)清除曝光优化器的梯度
                if use_sparse_adam:据 use_sparse_adam 的值，选择使用SparseGaussianAdam或标准的Adam优化器对高斯模型的参数进行更新

                    visible = radii > 0 获取可见高斯的索引
                    gaussians.optimizer.step(visible, radii.shape[0])使用稀疏Adam对高斯参数进行优化

                    gaussians.optimizer.zero_grad(set_to_none = True)清除所有参数的梯度，为下一轮迭代做准备

                else:
                    gaussians.optimizer.step()
                    gaussians.optimizer.zero_grad(set_to_none = True)

            if (iteration in checkpoint_iterations):
                print("\n[ITER {}] Saving Checkpoint".format(iteration))
                torch.save((gaussians.capture(), iteration), scene.model_path + "/chkpnt" + str(iteration) + ".pth")在 checkpoint_iterations 指定的迭代保存检查点，包含模型参数和当前迭代数，以便后续恢复训练

def prepare_output_and_logger(args):   负责设置训练结果的输出目录和TensorBoard日志

    if not args.model_path:如果没有指定模型输出路径
        if os.getenv('OAR_JOB_ID'):如果在 OAR 集群环境

            unique_str=os.getenv('OAR_JOB_ID')使用作业ID作为唯一字符串

        else:
            unique_str = str(uuid.uuid4())否则生成一个随机UUID

        args.model_path = os.path.join("./output/", unique_str[0:10])将输出路径设置为 ./output/unique_id
        
    # Set up output folder
    print("Output folder: {}".format(args.model_path))
    os.makedirs(args.model_path, exist_ok = True)创建输出文件夹

    with open(os.path.join(args.model_path, "cfg_args"), 'w') as cfg_log_f:
        cfg_log_f.write(str(Namespace(**vars(args))))将命令行参数保存到配置文件中


    # Create Tensorboard writer
    tb_writer = None
    if TENSORBOARD_FOUND:
        tb_writer = SummaryWriter(args.model_path)
    else:
        print("Tensorboard not available: not logging progress")
    return tb_writer

def training_report(tb_writer, iteration, Ll1, loss, l1_loss, elapsed, testing_iterations, scene : Scene, renderFunc, renderArgs, train_test_exp):负责记录训练过程中的各种指标和进行定期评估

    if tb_writer:
        tb_writer.add_scalar('train_loss_patches/l1_loss', Ll1.item(), iteration)记录训练 L1 损失

        tb_writer.add_scalar('train_loss_patches/total_loss', loss.item(), iteration)记录训练总损失

        tb_writer.add_scalar('iter_time', elapsed, iteration)记录每次迭代时间

    # Report test and samples of training set
    if iteration in testing_iterations:如果当前迭代在测试迭代列表中

        torch.cuda.empty_cache()清理CUDA缓存
        validation_configs = ({'name': 'test', 'cameras' : scene.getTestCameras()}, 测试集相机
                              {'name': 'train', 'cameras' : [scene.getTrainCameras()[idx % len(scene.getTrainCameras())] for idx in range(5, 30, 5)]})训练集样本相机

        for config in validation_configs:
            if config['cameras'] and len(config['cameras']) > 0:
                l1_test = 0.0
                psnr_test = 0.0
                for idx, viewpoint in enumerate(config['cameras']):
                    image = torch.clamp(renderFunc(viewpoint, scene.gaussians, *renderArgs)["render"], 0.0, 1.0)渲染测试图像

                    gt_image = torch.clamp(viewpoint.original_image.to("cuda"), 0.0, 1.0)获取测试真值图像

                    if train_test_exp:某些特定数据集可能需要裁剪图像

                        image = image[..., image.shape[-1] // 2:]
                        gt_image = gt_image[..., gt_image.shape[-1] // 2:]
                    if tb_writer and (idx < 5):记录前5个测试视图到TensorBoard

                        tb_writer.add_images(config['name'] + "_view_{}/render".format(viewpoint.image_name), image[None], global_step=iteration)记录渲染图像

                        if iteration == testing_iterations[0]:
                            tb_writer.add_images(config['name'] + "_view_{}/ground_truth".format(viewpoint.image_name), gt_image[None], global_step=iteration)记录真值图像 (只在第一次测试迭代)

                    l1_test += l1_loss(image, gt_image).mean().double()累加 L1 损失
                    psnr_test += psnr(image, gt_image).mean().double()累加 PSNR

                psnr_test /= len(config['cameras'])计算平均 PSNR
                l1_test /= len(config['cameras'])          
                print("\n[ITER {}] Evaluating {}: L1 {} PSNR {}".format(iteration, config['name'], l1_test, psnr_test))
                if tb_writer:
                    tb_writer.add_scalar(config['name'] + '/loss_viewpoint - l1_loss', l1_test, iteration)记录测试 L1 损失

                    tb_writer.add_scalar(config['name'] + '/loss_viewpoint - psnr', psnr_test, iteration)记录测试 PSNR

        if tb_writer:
            tb_writer.add_histogram("scene/opacity_histogram", scene.gaussians.get_opacity, iteration)记录不透明度直方图

            tb_writer.add_scalar('total_points', scene.gaussians.get_xyz.shape[0], iteration)记录高斯点数量

        torch.cuda.empty_cache()清理CUDA缓存

if __name__ == "__main__":
    # Set up command line argument parser
    parser = ArgumentParser(description="Training script parameters")
    lp = ModelParams(parser)模型参数

    op = OptimizationParams(parser)优化参数

    pp = PipelineParams(parser)渲染管线参数

    parser.add_argument('--ip', type=str, default="127.0.0.1")GUI服务器IP

    parser.add_argument('--port', type=int, default=6009)GUI服务器端口

    parser.add_argument('--debug_from', type=int, default=-1)GUI服务器端口

    parser.add_argument('--detect_anomaly', action='store_true', default=False)是否检测 PyTorch 自动微分异常

    parser.add_argument("--test_iterations", nargs="+", type=int, default=[7_000, 30_000])测试迭代列表

    parser.add_argument("--save_iterations", nargs="+", type=int, default=[7_000, 30_000])模型保存迭代列表

    parser.add_argument("--quiet", action="store_true")静默模式

    parser.add_argument('--disable_viewer', action='store_true', default=False)是否禁用GUI查看器

    parser.add_argument("--checkpoint_iterations", nargs="+", type=int, default=[])检查点保存迭代列表

    parser.add_argument("--start_checkpoint", type=str, default = None)恢复训练的检查点路径

    args = parser.parse_args(sys.argv[1:])解析命令行参数

    args.save_iterations.append(args.iterations)确保最后一迭代也保存模型
    
    print("Optimizing " + args.model_path)打印输出路径

    # Initialize system state (RNG)
    safe_state(args.quiet)设置随机种子，确保结果可复现

    # Start GUI server, configure and run training
    if not args.disable_viewer:
        network_gui.init(args.ip, args.port)初始化GUI服务器

    torch.autograd.set_detect_anomaly(args.detect_anomaly)设置是否检测梯度异常

    training(lp.extract(args), op.extract(args), pp.extract(args), args.test_iterations, args.save_iterations, args.checkpoint_iterations, args.start_checkpoint, args.debug_from)调用主训练函数

    # All done
    print("\nTraining complete.")

## 第五次学习记录
没学，改代码，已经要好哩
现在暂时的代码，效果还需要优化，优化三小时还是很一般啊，
import os
import cv2
import numpy as np
from PIL import Image, ImageOps
import torch
import shutil
import time
import traceback

try:
    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
except ImportError:
    print("未能导入 transformers 库。请确保已安装：pip install transformers accelerate")
    exit()

try:
    from segment_anything import sam_model_registry, SamPredictor
except ImportError:
    print("未能导入 segment_anything 库。请确保已安装")
    exit()

owlv2_model = None
owlv2_processor = None
sam_predictor = None
device = "cuda" if torch.cuda.is_available() else "cpu"

def load_models():
    global owlv2_model, owlv2_processor, sam_predictor
    owlv2_processor = AutoProcessor.from_pretrained("google/owlv2-base-patch16")
    owlv2_model = AutoModelForZeroShotObjectDetection.from_pretrained("google/owlv2-base-patch16").to(device)
    sam = sam_model_registry["vit_b"](checkpoint="sam_vit_b_01ec64.pth").to(device)
    sam_predictor = SamPredictor(sam)
    return True

def resize_image(image_pil, max_dim):
    w, h = image_pil.size
    scale = max_dim / max(w, h)
    new_size = (int(w * scale), int(h * scale))
    new_size = tuple([s // 16 * 16 for s in new_size])
    return image_pil.resize(new_size, Image.Resampling.LANCZOS)

def compute_iou(box1, box2):
    x1, y1, x2, y2 = box1
    x1_, y1_, x2_, y2_ = box2
    xi1, yi1 = max(x1, x1_), max(y1, y1_)
    xi2, yi2 = min(x2, x2_), min(y2, y2_)
    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
    area1 = (x2 - x1) * (y2 - y1)
    area2 = (x2_ - x1_) * (y2_ - y1_)
    union = area1 + area2 - inter_area
    return inter_area / union if union > 0 else 0

def nms(boxes, iou_thresh=0.5):
    boxes = sorted(boxes, key=lambda x: -x[1])
    keep = []
    for box in boxes:
        if all(compute_iou(box[0], kept[0]) < iou_thresh for kept in keep):
            keep.append(box)
    return keep

def get_all_detections(image_pil, prompts, score_thresh=0.1):
    all_results = []
    for prompt in prompts:
        for scale in [512, 768, 1024]:
            resized = resize_image(image_pil, scale)
            inputs = owlv2_processor(text=prompt, images=resized, return_tensors="pt").to(device)
            with torch.no_grad():
                outputs = owlv2_model(**inputs)
            results = owlv2_processor.post_process_object_detection(
                outputs, threshold=score_thresh,
                target_sizes=[[resized.height, resized.width]])[0]
            for box, score in zip(results['boxes'], results['scores']):
                box = box.cpu().numpy()
                scale_x = image_pil.width / resized.width
                scale_y = image_pil.height / resized.height
                box[[0, 2]] *= scale_x
                box[[1, 3]] *= scale_y
                all_results.append((box, float(score), prompt))
    return nms(all_results, iou_thresh=0.5)

def process_image(image_path, output_folder, prompts, score_thresh=0.2):
    image_pil = Image.open(image_path).convert("RGB")
    image_np = np.array(image_pil)
    sam_predictor.set_image(image_np)

    boxes = get_all_detections(image_pil, prompts, score_thresh)
    if not boxes:
        print(f"未检测到目标: {os.path.basename(image_path)}")
        return

    mask_all = np.zeros(image_np.shape[:2], dtype=bool)
    for box, score, label in boxes:
        masks, _, _ = sam_predictor.predict(box=box[None, :], point_coords=None, point_labels=None, multimask_output=False)
        if masks is None or masks.shape[0] == 0:
            continue
        mask = masks[0]
        if np.sum(mask) < 50:
            continue
        mask_all |= mask

    if not np.any(mask_all):
        print("掩码无效:", os.path.basename(image_path))
        return

    kernel = np.ones((5, 5), np.uint8)
    mask_all = cv2.morphologyEx(mask_all.astype(np.uint8), cv2.MORPH_CLOSE, kernel)
    mask_all = cv2.morphologyEx(mask_all, cv2.MORPH_OPEN, kernel)
    rgba = np.dstack([image_np, mask_all * 255])

    base = os.path.splitext(os.path.basename(image_path))[0]
    Image.fromarray(rgba).save(os.path.join(output_folder, base + "_processed.png"))
    print("保存成功:", base + "_processed.png")

def batch_process_images(input_dir, output_dir, prompts, thresh):
    if not load_models():
        return
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)

    for fname in os.listdir(input_dir):
        if fname.lower().endswith((".png", ".jpg", ".jpeg", ".bmp")):
            process_image(os.path.join(input_dir, fname), output_dir, prompts, thresh)

if __name__ == "__main__":
    input_dir = "C:/Users/admin/Desktop/vscode1/saibo/gaussian-splatting/qiu"
    output_dir = "C:/Users/admin/Desktop/vscode1/saibo/gaussian-splatting/tichu_images"
    print("请输入要保留的物体（多个以逗号/空格分隔）：")
    raw = input("提示词: ")
    prompts = [p.strip() for p in raw.replace(';', ',').replace(' ', ',').split(',') if p.strip()]
    batch_process_images(input_dir, output_dir, prompts, 0.2)
