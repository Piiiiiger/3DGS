## 第二周的学习
## 第一次学习记录
<font color=royalblue>gaussian_render/init.py  将 3D 高斯点投影到 2D 图像平面，并进行混合以生成最终的图像。 </font>:
import torch
import math
from diff_gaussian_rasterization import GaussianRasterizationSettings, GaussianRasterizer
导入核心的 CUDA 加速栅格化模块

from scene.gaussian_model import GaussianModel
from utils.sh_utils import eval_sh  导入用于评估球谐函数,eval_sh能根据视角方向和 SH 系数计算出当前视角的颜色。

def render(viewpoint_camera, pc : GaussianModel, pipe, bg_color : torch.Tensor, scaling_modifier = 1.0, separate_sh = False, override_color = None, use_trained_exp=False):
    """
    Render the scene. 
    
    Background tensor (bg_color) must be on GPU!
    """
 
    # Create zero tensor. We will use it to make pytorch return gradients of the 2D (screen-space) means
    screenspace_points = torch.zeros_like(pc.get_xyz, dtype=pc.get_xyz.dtype, requires_grad=True, device="cuda") + 0
    创建一个与 3D 高斯点位置 pc.get_xyz 形状相同的零张量，+ 0 是为了创建一个新的张量副本，而不是直接修改 pc.get_xyz。


    try:
        screenspace_points.retain_grad()
        retain_grad() 确保即使在 backward() 之后，screenspace_points 的梯度仍然可用。

    except:
        pass

    # Set up rasterization configuration
    tanfovx = math.tan(viewpoint_camera.FoVx * 0.5)

    tanfovy = math.tan(viewpoint_camera.FoVy * 0.5)
    将相机的水平和垂直视场角（Field of View, FoV）转换为其一半的反正切值。这是栅格化器所需的参数，用于确定投影的几何关系。tan(FoV/2) = (传感器尺寸/2) / 像距，其中传感器尺寸和像距单位需一致‌


    raster_settings = GaussianRasterizationSettings(
        image_height=int(viewpoint_camera.image_height),
        image_width=int(viewpoint_camera.image_width),
        tanfovx=tanfovx,
        tanfovy=tanfovy,
        视场角的正切值，决定了投影的透视效果。

        bg=bg_color,用于填充没有高斯点覆盖的区域。

        scale_modifier=scaling_modifier,高斯点尺度的全局调整系数。

        viewmatrix=viewpoint_camera.world_view_transform,世界坐标系到相机坐标系的变换矩阵

        projmatrix=viewpoint_camera.full_proj_transform,投影矩阵，将相机坐标系下的 3D 点投影到 2D 屏幕空间

        sh_degree=pc.active_sh_degree,

        campos=viewpoint_camera.camera_center,相机在世界坐标系中的位置，用于计算视角方向

        prefiltered=False,
        debug=pipe.debug,
        antialiasing=pipe.antialiasing抗锯齿开关。
    )
    实例化 GaussianRasterizationSettings 对象，传入所有渲染所需的相机参数和管线配置，

    rasterizer = GaussianRasterizer(raster_settings=raster_settings)实例化 GaussianRasterizer 对象，传入配置好的 raster_settings


    means3D = pc.get_xyz
    means2D = screenspace_points
    opacity = pc.get_opacity
    从 GaussianModel (pc) 中获取 3D 位置 (means3D)、2D 屏幕空间位置（用于梯度计算）和不透明度

    # If precomputed 3d covariance is provided, use it. If not, then it will be computed from
    # scaling / rotation by the rasterizer.
    scales = None
    rotations = None
    cov3D_precomp = None

    if pipe.compute_cov3D_python:如果在 Python 中计算 3D 协方差，则调用pc.get_covariance

        cov3D_precomp = pc.get_covariance(scaling_modifier)
    else:否则，栅格化器会在 CUDA 内部根据尺度和旋转计算 3D 协方差
        scales = pc.get_scaling
        rotations = pc.get_rotation

    # If precomputed colors are provided, use them. Otherwise, if it is desired to precompute colors
    # from SHs in Python, do it. If not, then SH -> RGB conversion will be done by rasterizer.
     高斯点的颜色可以由球谐函数（SH）计算得到，也可以直接提供预计算的 RGB 颜色
    shs = None
    colors_precomp = None用来存储预计算的 RGB 颜色
    if override_color is None: 如果没有指定覆盖颜色

        if pipe.convert_SHs_python:在没有 override_color 的情况下，进一步检查渲染管线参数 pipe 中是否设置了 convert_SHs_python ，决定了 SH 到 RGB 的转换是在 Python 代码中进行，还是在底层的 CUDA 栅格化器内部进行

            shs_view = pc.get_features.transpose(1, 2).view(-1, 3, (pc.max_sh_degree+1)**2)先获取斯模型 pc 中存储的SH 系数，再交换张量的第 1 维和第 2 维，然后重塑张量，-1 表示 PyTorch 会自动计算该维度的大小，3 表示 RGB 三个颜色通道


            dir_pp = (pc.get_xyz - viewpoint_camera.camera_center.repeat(pc.get_features.shape[0], 1))先获取所有高斯点的 3D 坐标，再获取当前相机在世界坐标系中的 3D 坐标，将相机中心坐标重复pc.get_features.shape[0] 次（即高斯点的数量），使其与 pc.get_xyz 的维度匹配，便于逐点相减，得到从每个高斯点指向相机中心的向量

            dir_pp_normalized = dir_pp/dir_pp.norm(dim=1, keepdim=True)先计算 dir_pp 中每个向量的 L2 范数（长度），dim=1 表示对行进行范数计算，keepdim=True 保持维度，将 dir_pp 中的每个向量进行归一化，使其长度为 1。

            sh2rgb = eval_sh(pc.active_sh_degree, shs_view, dir_pp_normalized)调用 eval_sh 函数根据相机方向评估 SH，得到当前视角的颜色

            colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)返回的 SH 颜色值通常在 -0.5 到 0.5 之间。将其加上 0.5，可以将其范围映射到 0 到 1 之间映射到 RGB 颜色范围（0 到 1）再将所有小于 0.0 的值裁剪为 0.0，计算出的 SH 颜色值从数学范围映射到有效的 RGB 颜色范围 [0, 1]，确保颜色是非负的，并且在栅格化时能够正确显示
        else:SH 到 RGB 的转换将在 CUDA 栅格化器内部进行

            if separate_sh:检查 SH 系数是否被分离存储

                dc, shs = pc.get_features_dc, pc.get_features_rest 如果分离存储，则获取 DC 分量和其余的 SH 分量。

            else:
                shs = pc.get_features  直接获取所有 SH 系数作为 shs

    else:
        colors_precomp = override_color  直接将传入的 override_color 赋值给 colors_precomp，如果外部已经指定了颜色，就直接使用它，而不再进行 SH 评估

    # Rasterize visible Gaussians to image, obtain their radii (on screen). 将可见的高斯点栅格化到图像上，并获取它们在屏幕上的半径高度，这是并行化的 GPU 操作，3D到2D投影: 将每3D高斯点（由 means3D、scales、rotations/cov3D_precomp 定义）投影到相机平面，形成一个2D 高斯，然后根据投影后的高斯点中心到相机的深度进行排序。为了正确的透明度混合，需要从后往前（或从前往后）处理高斯点，对于图像中的每个像素，遍历与其重叠的 2D 高斯椭圆。根据每个高斯点的不透明度 (opacities) 和颜色 (dc/shs/colors_precomp)，使用 alpha 混合公式将它们叠加起来，计算最终的像素颜色（体渲染公式）。

    if separate_sh:检查球谐函数（SH）的直流（DC）分量和高阶分量是否是分开存储和处理的
        rendered_image, radii, depth_image = rasterizer(
            means3D = means3D,高斯点的 3D 中心坐标

            means2D = means2D,斯点在屏幕空间中的 2D 投影中心。这个参数虽然在 CUDA 内部计算，但传入一个 requires_grad=True 的张量是为了捕获从 2D 屏幕空间到 3D 均值的梯度

            dc = dc,仅当 separate_sh 为 True 时提供

            shs = shs,球谐函数的高阶分量

            colors_precomp = colors_precomp,预计算的 RGB 颜色。如果 pipe.convert_SHs_python 为 True 或 override_color 存在，那么颜色会在这里直接提供，shs 将不被使用

            opacities = opacity,每个高斯点的不透明度。这决定了高斯点在混合时的权重

            scales = scales,高斯点的 3D 尺度向量。描述高斯椭球体的三个主轴长度

            rotations = rotations,旋转四元数

            cov3D_precomp = cov3D_precomp预计算的 3D 协方差矩阵。如果提供了这个参数，栅格化器会直接使用它，而不是从 scales 和 rotations 动态计算
            )
            将所有必要的高斯点属性和渲染配置传递给 rasterizer 对象

    else:SH 系数没有被分离存储，栅格化器会根据传入的 shs 参数（如果 colors_precomp 为 None）在内部完成 SH 到 RGB 的转换和颜色计算。

        rendered_image, radii, depth_image = rasterizer(
            means3D = means3D,
            means2D = means2D,
            shs = shs,
            colors_precomp = colors_precomp,
            opacities = opacity,
            scales = scales,
            rotations = rotations,
            cov3D_precomp = cov3D_precomp)
        
    # Apply exposure to rendered image (training only)
    对渲染图像应用曝光调整，并且通常只在训练阶段进行

    if use_trained_exp:
    如果 use_trained_exp 为 True，则执行曝光调整逻辑，而use_trained_exp 是一个布尔标志，控制是否应用训练好的曝光参数。在测试或最终渲染时，可能不需要应用这个动态曝光，或者会使用一个固定的曝光，用来根据训练/测试阶段或特定需求来控制曝光调整。

        exposure = pc.get_exposure_from_name(viewpoint_camera.image_name)
        从高斯模型 pc 中获取与当前相机视图对应的曝光参数。这个函数会根据 viewpoint_camera.image_name查找预先学习到的曝光值，检索与当前渲染视图关联的、由模型训练得到的曝光变换矩阵，曝光参数通常被建模为一个4x4的齐次变换矩阵，其中左上角的3x3子矩阵表示颜色空间的线性变换，最后一列的3x1向量表示亮度偏移

        rendered_image = torch.matmul(rendered_image.permute(1, 2, 0), exposure[:3, :3]).permute(2, 0, 1) + exposure[:3, 3,   None, None]
        先将将 rendered_image 的维度从 [C, H, W] (通道, 高度, 宽度) 变换到 [H, W, C]，让进行矩阵乘法时，将颜色通道作为最后一个维度，在获取获取 exposure 矩阵的左上角3x3部分，这是颜色空间的线性变换矩阵，再用torch.matmul执行矩阵乘法，渲染图像的每个像素的 RGB 向量 [R, G, B] 与3x3的颜色变换矩阵相乘，实现颜色空间变换，再将结果维度再次从 [H, W, C] 变换回 [C, H, W]，以恢复图像的常用格式再与亮度偏移相加 获取 exposure 矩阵的最后一列的3x1向量（表示平移/亮度偏移），并通过 None 扩展维度，使其形状变为 [3, 1, 1]，将学到的曝光变换（包括颜色空间调整和亮度偏移）应用到渲染图像上，使其更接近真实的图像条件。

    # Those Gaussians that were frustum culled or had a radius of 0 were not visible.
    # They will be excluded from value updates used in the splitting criteria.
    有些高斯点不可见：它们可能在相机的视锥体之外（被裁剪），或者它们的投影半径为零，它们将从用于分裂/克隆高斯点的统计数据中排除，不可见的高斯点对当前渲染结果没有贡献，因此它们的梯度为零。在自适应稠密化和剪枝过程中，只关心那些对图像有实际贡献的高斯点，以确保优化过程的效率和有效性。

    rendered_image = rendered_image.clamp(0, 1)将 rendered_image 的像素值限制在 [0, 1] 的范围内确保最终输出的图像具有有效的像素值，适合保存和显示

    out = {
        "render": rendered_image,最终渲染出的 RGB 图像

        "viewspace_points": screenspace_points,高斯点在屏幕空间中的 2D 投影点

        "visibility_filter" : (radii > 0).nonzero(),创建一个布尔张量，其中 True 表示对应的高斯点在屏幕上具有非零的投影半径（即可见），再返回布尔张量中所有 True 元素的索引

        "radii": radii,每个高斯点在屏幕空间中的投影半径。

        "depth" : depth_image渲染出的深度图
        
        }封装渲染函数的所有重要输出
    
    return out
