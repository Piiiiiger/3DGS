## 第二周的学习
## 第一次学习记录
<font color=royalblue>gaussian_render/init.py  将 3D 高斯点投影到 2D 图像平面，并进行混合以生成最终的图像。 </font>:
import torch
import math
from diff_gaussian_rasterization import GaussianRasterizationSettings, GaussianRasterizer
导入核心的 CUDA 加速栅格化模块

from scene.gaussian_model import GaussianModel
from utils.sh_utils import eval_sh  导入用于评估球谐函数,eval_sh能根据视角方向和 SH 系数计算出当前视角的颜色。

def render(viewpoint_camera, pc : GaussianModel, pipe, bg_color : torch.Tensor, scaling_modifier = 1.0, separate_sh = False, override_color = None, use_trained_exp=False):
    """
    Render the scene. 
    
    Background tensor (bg_color) must be on GPU!
    """
 
    # Create zero tensor. We will use it to make pytorch return gradients of the 2D (screen-space) means
    screenspace_points = torch.zeros_like(pc.get_xyz, dtype=pc.get_xyz.dtype, requires_grad=True, device="cuda") + 0
    创建一个与 3D 高斯点位置 pc.get_xyz 形状相同的零张量，+ 0 是为了创建一个新的张量副本，而不是直接修改 pc.get_xyz。


    try:
        screenspace_points.retain_grad()
        retain_grad() 确保即使在 backward() 之后，screenspace_points 的梯度仍然可用。

    except:
        pass

    # Set up rasterization configuration
    tanfovx = math.tan(viewpoint_camera.FoVx * 0.5)

    tanfovy = math.tan(viewpoint_camera.FoVy * 0.5)
    将相机的水平和垂直视场角（Field of View, FoV）转换为其一半的反正切值。这是栅格化器所需的参数，用于确定投影的几何关系。tan(FoV/2) = (传感器尺寸/2) / 像距，其中传感器尺寸和像距单位需一致‌


    raster_settings = GaussianRasterizationSettings(
        image_height=int(viewpoint_camera.image_height),
        image_width=int(viewpoint_camera.image_width),
        tanfovx=tanfovx,
        tanfovy=tanfovy,
        视场角的正切值，决定了投影的透视效果。

        bg=bg_color,用于填充没有高斯点覆盖的区域。

        scale_modifier=scaling_modifier,高斯点尺度的全局调整系数。

        viewmatrix=viewpoint_camera.world_view_transform,世界坐标系到相机坐标系的变换矩阵

        projmatrix=viewpoint_camera.full_proj_transform,投影矩阵，将相机坐标系下的 3D 点投影到 2D 屏幕空间

        sh_degree=pc.active_sh_degree,

        campos=viewpoint_camera.camera_center,相机在世界坐标系中的位置，用于计算视角方向

        prefiltered=False,
        debug=pipe.debug,
        antialiasing=pipe.antialiasing抗锯齿开关。
    )
    实例化 GaussianRasterizationSettings 对象，传入所有渲染所需的相机参数和管线配置，

    rasterizer = GaussianRasterizer(raster_settings=raster_settings)实例化 GaussianRasterizer 对象，传入配置好的 raster_settings


    means3D = pc.get_xyz
    means2D = screenspace_points
    opacity = pc.get_opacity
    从 GaussianModel (pc) 中获取 3D 位置 (means3D)、2D 屏幕空间位置（用于梯度计算）和不透明度

    # If precomputed 3d covariance is provided, use it. If not, then it will be computed from
    # scaling / rotation by the rasterizer.
    scales = None
    rotations = None
    cov3D_precomp = None

    if pipe.compute_cov3D_python:如果在 Python 中计算 3D 协方差，则调用pc.get_covariance

        cov3D_precomp = pc.get_covariance(scaling_modifier)
    else:否则，栅格化器会在 CUDA 内部根据尺度和旋转计算 3D 协方差
        scales = pc.get_scaling
        rotations = pc.get_rotation

    # If precomputed colors are provided, use them. Otherwise, if it is desired to precompute colors
    # from SHs in Python, do it. If not, then SH -> RGB conversion will be done by rasterizer.
     高斯点的颜色可以由球谐函数（SH）计算得到，也可以直接提供预计算的 RGB 颜色
    shs = None
    colors_precomp = None用来存储预计算的 RGB 颜色
    if override_color is None: 如果没有指定覆盖颜色

        if pipe.convert_SHs_python:在没有 override_color 的情况下，进一步检查渲染管线参数 pipe 中是否设置了 convert_SHs_python ，决定了 SH 到 RGB 的转换是在 Python 代码中进行，还是在底层的 CUDA 栅格化器内部进行

            shs_view = pc.get_features.transpose(1, 2).view(-1, 3, (pc.max_sh_degree+1)**2)先获取斯模型 pc 中存储的SH 系数，再交换张量的第 1 维和第 2 维，然后重塑张量，-1 表示 PyTorch 会自动计算该维度的大小，3 表示 RGB 三个颜色通道


            dir_pp = (pc.get_xyz - viewpoint_camera.camera_center.repeat(pc.get_features.shape[0], 1))先获取所有高斯点的 3D 坐标，再获取当前相机在世界坐标系中的 3D 坐标，将相机中心坐标重复pc.get_features.shape[0] 次（即高斯点的数量），使其与 pc.get_xyz 的维度匹配，便于逐点相减，得到从每个高斯点指向相机中心的向量

            dir_pp_normalized = dir_pp/dir_pp.norm(dim=1, keepdim=True)先计算 dir_pp 中每个向量的 L2 范数（长度），dim=1 表示对行进行范数计算，keepdim=True 保持维度，将 dir_pp 中的每个向量进行归一化，使其长度为 1。

            sh2rgb = eval_sh(pc.active_sh_degree, shs_view, dir_pp_normalized)调用 eval_sh 函数根据相机方向评估 SH，得到当前视角的颜色

            colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)返回的 SH 颜色值通常在 -0.5 到 0.5 之间。将其加上 0.5，可以将其范围映射到 0 到 1 之间映射到 RGB 颜色范围（0 到 1）再将所有小于 0.0 的值裁剪为 0.0，计算出的 SH 颜色值从数学范围映射到有效的 RGB 颜色范围 [0, 1]，确保颜色是非负的，并且在栅格化时能够正确显示
        else:SH 到 RGB 的转换将在 CUDA 栅格化器内部进行

            if separate_sh:检查 SH 系数是否被分离存储

                dc, shs = pc.get_features_dc, pc.get_features_rest 如果分离存储，则获取 DC 分量和其余的 SH 分量。

            else:
                shs = pc.get_features  直接获取所有 SH 系数作为 shs

    else:
        colors_precomp = override_color  直接将传入的 override_color 赋值给 colors_precomp，如果外部已经指定了颜色，就直接使用它，而不再进行 SH 评估

    # Rasterize visible Gaussians to image, obtain their radii (on screen). 将可见的高斯点栅格化到图像上，并获取它们在屏幕上的半径高度，这是并行化的 GPU 操作，3D到2D投影: 将每3D高斯点（由 means3D、scales、rotations/cov3D_precomp 定义）投影到相机平面，形成一个2D 高斯，然后根据投影后的高斯点中心到相机的深度进行排序。为了正确的透明度混合，需要从后往前（或从前往后）处理高斯点，对于图像中的每个像素，遍历与其重叠的 2D 高斯椭圆。根据每个高斯点的不透明度 (opacities) 和颜色 (dc/shs/colors_precomp)，使用 alpha 混合公式将它们叠加起来，计算最终的像素颜色（体渲染公式）。

    if separate_sh:检查球谐函数（SH）的直流（DC）分量和高阶分量是否是分开存储和处理的
        rendered_image, radii, depth_image = rasterizer(
            means3D = means3D,高斯点的 3D 中心坐标

            means2D = means2D,斯点在屏幕空间中的 2D 投影中心。这个参数虽然在 CUDA 内部计算，但传入一个 requires_grad=True 的张量是为了捕获从 2D 屏幕空间到 3D 均值的梯度

            dc = dc,仅当 separate_sh 为 True 时提供

            shs = shs,球谐函数的高阶分量

            colors_precomp = colors_precomp,预计算的 RGB 颜色。如果 pipe.convert_SHs_python 为 True 或 override_color 存在，那么颜色会在这里直接提供，shs 将不被使用

            opacities = opacity,每个高斯点的不透明度。这决定了高斯点在混合时的权重

            scales = scales,高斯点的 3D 尺度向量。描述高斯椭球体的三个主轴长度

            rotations = rotations,旋转四元数

            cov3D_precomp = cov3D_precomp预计算的 3D 协方差矩阵。如果提供了这个参数，栅格化器会直接使用它，而不是从 scales 和 rotations 动态计算
            )
            将所有必要的高斯点属性和渲染配置传递给 rasterizer 对象

    else:SH 系数没有被分离存储，栅格化器会根据传入的 shs 参数（如果 colors_precomp 为 None）在内部完成 SH 到 RGB 的转换和颜色计算。

        rendered_image, radii, depth_image = rasterizer(
            means3D = means3D,
            means2D = means2D,
            shs = shs,
            colors_precomp = colors_precomp,
            opacities = opacity,
            scales = scales,
            rotations = rotations,
            cov3D_precomp = cov3D_precomp)
        
    # Apply exposure to rendered image (training only)
    对渲染图像应用曝光调整，并且通常只在训练阶段进行

    if use_trained_exp:
    如果 use_trained_exp 为 True，则执行曝光调整逻辑，而use_trained_exp 是一个布尔标志，控制是否应用训练好的曝光参数。在测试或最终渲染时，可能不需要应用这个动态曝光，或者会使用一个固定的曝光，用来根据训练/测试阶段或特定需求来控制曝光调整。

        exposure = pc.get_exposure_from_name(viewpoint_camera.image_name)
        从高斯模型 pc 中获取与当前相机视图对应的曝光参数。这个函数会根据 viewpoint_camera.image_name查找预先学习到的曝光值，检索与当前渲染视图关联的、由模型训练得到的曝光变换矩阵，曝光参数通常被建模为一个4x4的齐次变换矩阵，其中左上角的3x3子矩阵表示颜色空间的线性变换，最后一列的3x1向量表示亮度偏移

        rendered_image = torch.matmul(rendered_image.permute(1, 2, 0), exposure[:3, :3]).permute(2, 0, 1) + exposure[:3, 3,   None, None]
        先将将 rendered_image 的维度从 [C, H, W] (通道, 高度, 宽度) 变换到 [H, W, C]，让进行矩阵乘法时，将颜色通道作为最后一个维度，在获取获取 exposure 矩阵的左上角3x3部分，这是颜色空间的线性变换矩阵，再用torch.matmul执行矩阵乘法，渲染图像的每个像素的 RGB 向量 [R, G, B] 与3x3的颜色变换矩阵相乘，实现颜色空间变换，再将结果维度再次从 [H, W, C] 变换回 [C, H, W]，以恢复图像的常用格式再与亮度偏移相加 获取 exposure 矩阵的最后一列的3x1向量（表示平移/亮度偏移），并通过 None 扩展维度，使其形状变为 [3, 1, 1]，将学到的曝光变换（包括颜色空间调整和亮度偏移）应用到渲染图像上，使其更接近真实的图像条件。

    # Those Gaussians that were frustum culled or had a radius of 0 were not visible.
    # They will be excluded from value updates used in the splitting criteria.
    有些高斯点不可见：它们可能在相机的视锥体之外（被裁剪），或者它们的投影半径为零，它们将从用于分裂/克隆高斯点的统计数据中排除，不可见的高斯点对当前渲染结果没有贡献，因此它们的梯度为零。在自适应稠密化和剪枝过程中，只关心那些对图像有实际贡献的高斯点，以确保优化过程的效率和有效性。

    rendered_image = rendered_image.clamp(0, 1)将 rendered_image 的像素值限制在 [0, 1] 的范围内确保最终输出的图像具有有效的像素值，适合保存和显示

    out = {
        "render": rendered_image,最终渲染出的 RGB 图像

        "viewspace_points": screenspace_points,高斯点在屏幕空间中的 2D 投影点

        "visibility_filter" : (radii > 0).nonzero(),创建一个布尔张量，其中 True 表示对应的高斯点在屏幕上具有非零的投影半径（即可见），再返回布尔张量中所有 True 元素的索引

        "radii": radii,每个高斯点在屏幕空间中的投影半径。

        "depth" : depth_image渲染出的深度图
        
        }封装渲染函数的所有重要输出
    
    return out

## 第二次学习记录
<font color="royalblue">代码中的camera类，承载了渲染一张图像所需的所有几何信息和图像本身的数据，定义了“一个相机是如何看世界”以及“这张视角下的原始图像长什么样”，是 3DGS 渲染算法的直接操作对象</font>

import torch
from torch import nn
import numpy as np
from utils.graphics_utils import getWorld2View2, getProjectionMatrix
from utils.general_utils import PILtoTorch
import cv2

class Camera(nn.Module):
    def __init__(self, resolution, colmap_id, R, T, FoVx, FoVy, depth_params, image, invdepthmap,
                 image_name, uid,
                 trans=np.array([0.0, 0.0, 0.0]), scale=1.0, data_device = "cuda",
                 train_test_exp = False, is_test_dataset = False, is_test_view = False
                 ):
        R是旋转矩阵（3x3 NumPy 数组），表示相机在世界坐标系中的方向。它将点从世界坐标系旋转到相机坐标系。
        T是平移向量（3x1 NumPy 数组），表示相机在世界坐标系中的位置。它将点从世界坐标系平移到相机坐标系
        depth_params是一个字典，包含与深度图缩放和偏移相关的参数
        invdepthmap是一个 NumPy 数组，表示此相机视图的逆深度图。如果不可用，则为 None


        super(Camera, self).__init__()

        self.uid = uid
        self.colmap_id = colmap_id
        self.R = R
        self.T = T
        self.FoVx = FoVx
        self.FoVy = FoVy
        self.image_name = image_name

        try:
            self.data_device = torch.device(data_device)
        except Exception as e:
            print(e)
            print(f"[Warning] Custom device {data_device} failed, fallback to default cuda device" )
            self.data_device = torch.device("cuda")

        resized_image_rgb = PILtoTorch(image, resolution)将 PIL 图像转换为 PyTorch 张量，并可能将其大小调整为 resolution
        从 resized_image_rgb 张量中提取 RGB 通道（前 3 个通道）

        self.alpha_mask = None  

        
        if resized_image_rgb.shape[0] == 4:如果输入图像有 4 个通道 (RGBA)，则提取alpha通道，并将alpha掩码张量移动到指定的计算设

            self.alpha_mask = resized_image_rgb[3:4, ...].to(self.data_device)

        else: 如果图像只有 3 个通道 (RGB)，则创建一个全为 1 的掩码，将alpha掩码张量移动到指定的计算设。这意味着所有像素都被视为不透明或有效，

            self.alpha_mask = torch.ones_like(resized_image_rgb[0:1, ...].to(self.data_device))

        通过隐藏真实图像的一部分，可以测试模型合成新视图或填充缺失信息的能力
        if train_test_exp and is_test_view:如果 train_test_exp 为真且这是 is_test_view

            if is_test_dataset:如果is_test_dataset也为真，则图像 alpha 掩码的左半部分设置为 0，有效地使这些像素在渲染或损失计算期间透明
                self.alpha_mask[..., :self.alpha_mask.shape[-1] // 2] = 0
            else:如果 is_test_dataset 为假，意味着它是来自训练数据集的测试视图，则图像alpha掩码的右半部分设置为 0
                self.alpha_mask[..., self.alpha_mask.shape[-1] // 2:] = 0

        self.original_image = gt_image.clamp(0.0, 1.0).to(self.data_device)
        self.image_width = self.original_image.shape[2]
        self.image_height = self.original_image.shape[1]

        self.invdepthmap = None
        self.depth_reliable = False
        if invdepthmap is not None:
            self.depth_mask = torch.ones_like(self.alpha_mask)创建一个全为 1 的掩码，最初假设所有深度值都是可靠的
            self.invdepthmap = cv2.resize(invdepthmap, resolution)使用 OpenCV 将输入的逆深度图调整到目标分辨率
            self.invdepthmap[self.invdepthmap < 0] = 0将负的逆深度值钳制为零
            self.depth_reliable = True 深度信息可用且最初被认为是可靠的

            if depth_params is not None:
                if depth_params["scale"] < 0.2 * depth_params["med_scale"] or depth_params["scale"] > 5 * depth_params  ["med_scale"]:  检查scale参数，太大或太小将depth_reliable标志设置为False，并将depth_mask设置为全零，过滤掉可能错误或无用的深度图
                    self.depth_reliable = False
                    self.depth_mask *= 0
                
                if depth_params["scale"] > 0: 为正，则对invdepthmap进行缩放和偏移
                    self.invdepthmap = self.invdepthmap * depth_params["scale"] + depth_params["offset"]

            if self.invdepthmap.ndim != 2:确保逆深度图是 2D 的
                self.invdepthmap = self.invdepthmap[..., 0]
            self.invdepthmap = torch.from_numpy(self.invdepthmap[None]).to(self.data_device)将NumPy数组转换为 PyTorch 张量，并添加一个批次维度

        self.zfar = 100.0
        self.znear = 0.01
        定义了相机视锥体的近裁剪面和远裁剪面。比znear近或比zfar远的物体通常会被裁剪而不被渲染


        self.trans = trans
        self.scale = scale
        平移和缩放因子

        self.world_view_transform = torch.tensor(getWorld2View2(R, T, trans, scale)).transpose(0, 1).cuda()将一个4x4矩阵，一个将3D点从世界坐标系转换为相机（视图）坐标系的矩阵的逆矩阵变成张量

        self.projection_matrix = getProjectionMatrix(znear=self.znear, zfar=self.zfar, fovX=self.FoVx, fovY=self.FoVy).transpose(0,1).cuda()计算投影矩阵，将3D点从相机（视图）坐标系转换为2D齐次裁剪坐标，矩阵用来定义视锥体和映射。
        裁剪坐标中的点通常通过其“w”分量（w是齐次坐标的第四个维度，因为在笛卡尔坐标系中，平移是一个加法运算，而旋转和缩放是乘法运算。这使得它们难以用一个统一的矩阵乘法来表示，引入 w 分量，可以将 3D 点 (x,y,z) 表示为 4D 齐次坐标 (x',y' ,z' ,w' ),然后实际的 3D 笛卡尔坐标是 (x'/w' ,y'/w  ,z'/w'),变成4x4的矩阵乘法能在图形编程中简化变换链的处理，因为你需要对一个物体依次进行平移、旋转和缩放操作，你可以分别构建平移矩阵T、旋转矩阵 R和缩放矩阵S，得到一个最终的组合变换矩阵 M,M =T ⋅ R ⋅ S）
        进行除法（透视除法）以获得归一化设备坐标 (NDC)，然后缩放/平移到屏幕坐标

        self.full_proj_transform = (self.world_view_transform.unsqueeze(0).bmm(self.projection_matrix.unsqueeze(0))).squeeze(0)
        这是世界到视图变换和投影矩阵的串联，这个单一的4x4矩阵可以直接将3D世界点转换为2D齐次裁剪坐标

        self.camera_center = self.world_view_transform.inverse()[3, :3]世界坐标系中的相机中心可以通过世界到视图变换矩阵的逆矩阵找到。逆矩阵的最后一列（或行，取决于约定），特别是前三个元素，将表示相机在世界坐标系中的位置
        
class MiniCam:
    def __init__(self, width, height, fovy, fovx, znear, zfar, world_view_transform, full_proj_transform):
        self.image_width = width
        self.image_height = height    
        self.FoVy = fovy
        self.FoVx = fovx
        self.znear = znear
        self.zfar = zfar

        self.world_view_transform = world_view_transform 预计算的世界到视图矩阵

        self.full_proj_transform = full_proj_transform 预计算的完整投影矩阵

        view_inv = torch.inverse(self.world_view_transform)计算世界到视图矩阵的逆矩阵，它将点从相机坐标系转换到世界坐标系

        self.camera_center = view_inv[3][:3]从逆矩阵中提取相机在世界坐标系中的位置