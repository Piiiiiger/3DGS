# 示例

# 第一周

## 第一次记录

3dgs类代码
class GaussianModel:

    def setup_functions(self):
        def build_covariance_from_scaling_rotation(scaling, scaling_modifier, rotation):
            L = build_scaling_rotation(scaling_modifier * scaling, rotation)
            actual_covariance = L @ L.transpose(1, 2)
            symm = strip_symmetric(actual_covariance)
            return symm
        
        self.scaling_activation = torch.exp
        self.scaling_inverse_activation = torch.log

        self.covariance_activation = build_covariance_from_scaling_rotation

        self.opacity_activation = torch.sigmoid
        self.inverse_opacity_activation = inverse_sigmoid

        self.rotation_activation = torch.nn.functional.normalize


    def __init__(self, sh_degree, optimizer_type="default"):
        self.active_sh_degree = 0
        self.optimizer_type = optimizer_type
        self.max_sh_degree = sh_degree  
        self._xyz = torch.empty(0)
        self._features_dc = torch.empty(0)
        self._features_rest = torch.empty(0)
        self._scaling = torch.empty(0)
        self._rotation = torch.empty(0)
        self._opacity = torch.empty(0)
        self.max_radii2D = torch.empty(0)
        self.xyz_gradient_accum = torch.empty(0)
        self.denom = torch.empty(0)
        self.optimizer = None
        self.percent_dense = 0
        self.spatial_lr_scale = 0
        self.setup_functions()

    def capture(self):
        return (
            self.active_sh_degree,
            self._xyz,
            self._features_dc,
            self._features_rest,
            self._scaling,
            self._rotation,
            self._opacity,
            self.max_radii2D,
            self.xyz_gradient_accum,
            self.denom,
            self.optimizer.state_dict(),
            self.spatial_lr_scale,
        )

    def restore(self, model_args, training_args):
        (self.active_sh_degree, 
        self._xyz, 
        self._features_dc, 
        self._features_rest,
        self._scaling, 
        self._rotation, 
        self._opacity,
        self.max_radii2D, 
        xyz_gradient_accum, 
        denom,
        opt_dict, 
        self.spatial_lr_scale) = model_args
        self.training_setup(training_args)
        self.xyz_gradient_accum = xyz_gradient_accum
        self.denom = denom
        self.optimizer.load_state_dict(opt_dict)

def build_covariance_from_scaling_rotation(scaling, scaling_modifier, rotation):构建协方差矩阵，先构建缩放矩阵L，再actual_covariance = L @ L.transpose(1, 2)计算协方差，分别传入的是缩放，缩放修正和旋转

接着就是指数函数用exp（x）为e的x次幂，保证缩放因子为正。

self.opacity_activation = torch.sigmoid，透明度[0,1]

上面的每一个运算都有逆运算

self.rotation_activation = torch.nn.functional.normalize,归一化函数，保持旋转四元数为单位四元数


下面是--init--
self.active_sh_degree = 0和self.max_sh_degree = sh_degree是球谐函数的最大阶和当前阶

self._xyz = torch.empty(0)，高斯的位置，用torch当前0阶作为参数，实际情况会变成传入的【N,3】矩阵，下面同样

self._features_dc = torch.empty(0) 球谐函数DC分量，表示基础颜色，【N,3】一般为（R，G,B）

self._features_rest = torch.empty(0) 球谐函数高频分量，根据最高阶来描述视角依赖的颜色变化为【N,(max_sh*max_sh -1)*3】，捕捉镜面反射、光泽效果，例如一阶的话是线性变化，二阶是二次变化

self._scaling = torch.empty(0)缩放参数[N，3]，控制高斯椭球在各轴的大小，决定协方差矩阵的特征值，形成缩放矩阵

self._rotation = torch.empty(0)旋转四元数[N, 4]，控制高斯椭球的方向，并且决定协方差矩阵的特征向量，高斯方向如果对着相机就是高贡献，背对即为低贡献

self._opacity = torch.empty(0)透明度参数[N,1]控制高斯点的可见性

self.max_radii2D = torch.empty(0)，最大2d投影半径，表示高斯点在图像平面上的覆盖范围，可以在渲染前用于剔除对当前视图贡献小的高斯点

self.xyz_gradient_accum = torch.empty(0) 位置梯度累积，[N, 3]对每个高斯点的xyz梯度累积

self.denom = torch.empty(0) 自适应过程中的分母项，每个高斯点一个值

self.optimizer = None 优化器

self.percent_dense = 0密集度百分比可以控制高斯点分裂或者克隆的强度，为【0，1】一般为0.01，前x%的高斯点，x为precent——dense,在自适应过程中的检查

self.spatial_lr_scale = 0 空间学习率缩放因子 根据高斯点对角线在背景的尺寸比例，消除场景尺寸对学习率的影响，防止大场景中小学习率导致收敛慢，小场景中大学习率导致震荡，学习率控制着模型在训练过程中更新权重的步长大小

def capture(self):
    return (
        self.active_sh_degree,
        self._xyz,
        self._features_dc,
        self._features_rest,
        self._scaling,
        self._rotation,
        self._opacity,
        self.max_radii2D,
        self.xyz_gradient_accum,
        self.denom,
        self.optimizer.state_dict(),  # 优化器状态
        self.spatial_lr_scale,
    )返回元组的状态，即作为一个接口获取传入元组的所有参数，用于保存检查点

def restore(self, model_args, training_args):
    (self.active_sh_degree, 
     self._xyz, 
     self._features_dc, 
     self._features_rest,
     self._scaling, 
     self._rotation, 
     self._opacity,
     self.max_radii2D, 
     xyz_gradient_accum, 
     denom,
     opt_dict, 
     self.spatial_lr_scale) = model_args
     
    self.training_setup(training_args)
    
    # 恢复优化状态
    self.xyz_gradient_accum = xyz_gradient_accum
    self.denom = denom
    self.optimizer.load_state_dict(opt_dict)
    ）实现训练中断后继续训练，先加载保存的模型状态，调用 training_setup 重新初始化优化器，然后恢复梯度累积等优化状态变量

## 第二次记录

下面是@property 装饰器定义，封装内部状态，提供一致接口，并且支持自动微分
    @property
    def get_scaling(self):获取激活后的缩放参数
        return self.scaling_activation(self._scaling)激活原始的参数
    
    @property
    def get_rotation(self):获取归一化后的旋转四元数
        return self.rotation_activation(self._rotation)
    
    @property
    def get_xyz(self):直接获取位置坐标
        return self._xyz
    
    @property
    def get_features(self):获取所有球谐系数
        features_dc = self._features_dc
        features_rest = self._features_rest
        return torch.cat((features_dc, features_rest), dim=1)用cat动态拼接DC和高阶分量
    
    @property
    def get_features_dc(self):获取球谐DC分量
        return self._features_dc
    
    @property
    def get_features_rest(self):获取球谐高阶分量
        return self._features_rest
    
    @property
    def get_opacity(self):获取激活后的不透明度
        return self.opacity_activation(self._opacity)
    
    @property
    def get_exposure(self):获取相机曝光参数，形状[N_cam, 3, 4]
        return self._exposure

def get_exposure_from_name(self, image_name):##曝光处理，获取特定图像的曝光参数

    if self.pretrained_exposures is None:##未训练
    
        return self._exposure[self.exposure_mapping[image_name]]#图像名称到参数索引的映射字典
    else:#预训练
    
        return self.pretrained_exposures[image_name]


def get_covariance(self, scaling_modifier = 1):##计算协方差矩阵

        return self.covariance_activation(self.get_scaling, scaling_modifier, self._rotation)


def oneupSHdegree(self):##提升球谐函数阶数，渐进式学习，先颜色，再反射，接着精细的反射

    if self.active_sh_degree < self.max_sh_degree:
    
        self.active_sh_degree += 1


def create_from_pcd(self, pcd : BasicPointCloud, cam_infos : int, spatial_lr_scale : float):点云初始化3D高斯模型
        self.spatial_lr_scale = spatial_lr_scale空间缩放
        fused_point_cloud = torch.tensor(np.asarray(pcd.points)).float().cuda()位置初始化，将点云位置转换为CUDA张量，np.asarray：转换为NumPy数组，torch.tensor：创建PyTorch张量，float()：转换为浮点类型，cuda()：移至GPU
        
        fused_color = RGB2SH(torch.tensor(np.asarray(pcd.colors)).float().cuda())将RGB转为球谐DC分量
        
        features = torch.zeros((fused_color.shape[0], 3, (self.max_sh_degree + 1) ** 2)).float().cuda()创建全零球谐系数张量
        
        features[:, :3, 0 ] = fused_color 将球谐DC分量放入第一个系数槽
        features[:, 3:, 1:] = 0.0 初始化所有高阶球谐系数为0

        print("Number of points at initialisation : ", fused_point_cloud.shape[0])

        dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)基于点间距计算初始缩放，在GPU上并行计算点间距，返回每个点的最小距离平方
        
        scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3) 缩放参数初始化，torch.log()：取对数 → 优化空间，[...,None]：增加维度 [N,1]，repeat(1,3)：复制到三个轴 [N,3]
        
        rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")初始化旋转四元数
        rots[:, 0] = 1 [1,0,0,0]

        opacities = self.inverse_opacity_activation(0.1 * torch.ones((fused_point_cloud.shape[0], 1), dtype=torch.float, device="cuda"))初始化透明度参数，创建全0.1张量 [N,1]，应用逆sigmoid

        self._xyz = nn.Parameter(fused_point_cloud.requires_grad_(True))将位置设为可训练参数，nn.Parameter 标记为模型参数，requires_grad_(True) 启用梯度
        
        self._features_dc = nn.Parameter(features[:,:,0:1].transpose(1, 2).contiguous().requires_grad_(True))注册球谐DC分量参数，切片获取DC分量 [N,3,1]，transpose(1,2) → [N,1,3]，contiguous() 确保内存连续
        
        self._features_rest = nn.Parameter(features[:,:,1:].transpose(1, 2).contiguous().requires_grad_(True))注册球谐高阶分量参数，[:,:,1:] 获取1阶及以上系数
        
        self._scaling = nn.Parameter(scales.requires_grad_(True))注册缩放参数，存储缩放后的对数值
        
        self._rotation = nn.Parameter(rots.requires_grad_(True))注册旋转四元数参数
        
        self._opacity = nn.Parameter(opacities.requires_grad_(True))注册透明度参数，存储逆simoid的值
        
        self.max_radii2D = torch.zeros((self.get_xyz.shape[0]), device="cuda")初始化2D投影半径为0
        
        self.exposure_mapping = {cam_info.image_name: idx for idx, cam_info in enumerate(cam_infos)}创建图像名到曝光参数索引的映射
        self.pretrained_exposures = None 标记无预训练曝光参数
        
        exposure = torch.eye(3, 4, device="cuda")[None].repeat(len(cam_infos), 1, 1)初始化曝光参数矩阵[N, 3, 4]
        
        self._exposure = nn.Parameter(exposure.requires_grad_(True))注册可学习的曝光参数


def training_setup(self, training_args):
        self.percent_dense = training_args.percent_dense设置密集度百分比，从训练参数获取密集度百分比值
        
        self.xyz_gradient_accum = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")创建并归零位置梯度累积张量，记录每个高斯点位置梯度的累积量
        
        self.denom = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")创建并归零分母项张量，记录每个高斯点的更新次数

        l = [
            {'params': [self._xyz], 'lr': training_args.position_lr_init * self.spatial_lr_scale, "name": "xyz"},
            {'params': [self._features_dc], 'lr': training_args.feature_lr, "name": "f_dc"},
            {'params': [self._features_rest], 'lr': training_args.feature_lr / 20.0, "name": "f_rest"},
            {'params': [self._opacity], 'lr': training_args.opacity_lr, "name": "opacity"},
            {'params': [self._scaling], 'lr': training_args.scaling_lr, "name": "scaling"},
            {'params': [self._rotation], 'lr': training_args.rotation_lr, "name": "rotation"}
        ]定义优化参数组，应用空间缩放因子，同时SH高频分量20倍降速降低学习率防止过拟合

        if self.optimizer_type == "default":
            self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)default (Adam），标准Adam优化器，通用场景
        elif self.optimizer_type == "sparse_adam":为稀疏梯度定制，大型场景(>100万高斯点)
            try:
                self.optimizer = SparseGaussianAdam(l, lr=0.0, eps=1e-15)
            except:
                # A special version of the rasterizer is required to enable sparse adam
                self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)

        self.exposure_optimizer = torch.optim.Adam([self._exposure])为曝光参数单独创建优化器，避免主优化器的干扰

        self.xyz_scheduler_args = get_expon_lr_func(lr_init=training_args.position_lr_init*self.spatial_lr_scale,
                                                    lr_final=training_args.position_lr_final*self.spatial_lr_scale,
                                                    lr_delay_mult=training_args.position_lr_delay_mult,
                                                    max_steps=training_args.position_lr_max_steps)位置学习率调度配置，初始学习率，最终学习率，延迟衰减因子，最大步数
        
        self.exposure_scheduler_args = get_expon_lr_func(training_args.exposure_lr_init, training_args.exposure_lr_final,
                                                        lr_delay_steps=training_args.exposure_lr_delay_steps,独立设置，曝光可能需要更早调整
                                                        lr_delay_mult=training_args.exposure_lr_delay_mult,
                                            全程调整曝光 max_steps=training_args.iterations)曝光学习率调度配置


## 第三次记录
def construct_list_of_attributes(self):
    l = ['x', 'y', 'z', 'nx', 'ny', 'nz']基础几何数据，nx，ny，nz是法线
    
    for i in range(self._features_dc.shape[1]*self._features_dc.shape[2]):self._features_dc 是形状为 (N, 1, 3) 的张量，1 * 3 = 3，生成f_dc_0, f_dc_1, f_dc_2，对应RGB三个颜色通道的基色分量
        l.append('f_dc_{}'.format(i))
    
    for i in range(self._features_rest.shape[1]*self._features_rest.shape[2]):self._features_rest 是形状为 (N, 15, 3) 的张量，循环次数 = 15 * 3 = 45
        l.append('f_rest_{}'.format(i))
    
    l.append('opacity')添加透明度
    
    for i in range(self._scaling.shape[1]):缩放三个分量
        l.append('scale_{}'.format(i))
    
    for i in range(self._rotation.shape[1]):旋转四个分量
        l.append('rot_{}'.format(i))
    
    return l


def save_ply(self, path):全流程为PyTorch张量变成CPU NumPy数组，再变成结构化数组，再变成PLY元素，最后变成PLY文件
        mkdir_p(os.path.dirname(path))

        xyz = self._xyz.detach().cpu().numpy()  detach()：从计算图中分离，避免梯度计算
        
        normals = np.zeros_like(xyz)初始化法线数据，创建与位置数组相同形状的全零数组，暂时用零填充
        
        f_dc = self._features_dc.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()   transpose(1, 2)：交换维度 → (N, 3, 1)，flatten(start_dim=1)从第1维展平 → (N, 3)
        
        f_rest = self._features_rest.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()  transpose(1, 2) → (N, 3, 15)  flatten(start_dim=1) → (N, 45) 3x15=45
        
        opacities = self._opacity.detach().cpu().numpy()

        scale = self._scaling.detach().cpu().numpy()提取缩放参数(N, 3)
        
        rotation = self._rotation.detach().cpu().numpy()提取旋转参数,(N, 4)

        dtype_full = [(attribute, 'f4') for attribute in self.construct_list_of_attributes()]  调用之前定义的construct_list_of_attributes()获取属性列表

        elements = np.empty(xyz.shape[0], dtype=dtype_full)创建空的结构化数组：
        
        attributes = np.concatenate((xyz, normals, f_dc, f_rest, opacities, scale, rotation), axis=1)合并所有属性,按列连接所有NumPy数组,连接顺序与construct_list_of_attributes()定义的顺序一致
        
        elements[:] = list(map(tuple, attributes)) 填充结构化数组,变成每行数据为元组的列表
        
        el = PlyElement.describe(elements, 'vertex') 创建PLY元素
        
        PlyData([el]).write(path)写入PLY文件



 def reset_opacity(self):
        opacities_new = self.inverse_opacity_activation(torch.min(self.get_opacity, torch.ones_like(self.get_opacity)*0.01))将当前不透明度值与0.01进行比较，取最小值，再将激活后的不透明度值转换回优化器使用的原始参数空间
        
        optimizable_tensors = self.replace_tensor_to_optimizer(opacities_new, "opacity")调用自定义方法替换优化器中的参数
        
        self._opacity = optimizable_tensors["opacity"]更新模型内部状态

def load_ply(self, path, use_train_test_exp = False):与save相反
        plydata = PlyData.read(path)使用plyfile库加载PLY文件数据
        
        if use_train_test_exp:构建曝光文件路径：假设在PLY文件的上两级目录
            exposure_file = os.path.join(os.path.dirname(path), os.pardir, os.pardir, "exposure.json")
            if os.path.exists(exposure_file):
                with open(exposure_file, "r") as f:
                    exposures = json.load(f)
                self.pretrained_exposures = {image_name: torch.FloatTensor(exposures[image_name]).requires_grad_(False).cuda() for image_name in exposures}
                print(f"Pretrained exposures loaded.")
            else:
                print(f"No exposure to be loaded at {exposure_file}")
                self.pretrained_exposures = None
          检查文件是否存在，如果存在：读取JSON格式的曝光数据，转换为PyTorch张量并移动到GPU，禁用梯度计算（requires_grad_(False)），按图像名称存储到self.pretrained_exposures
          如果不存在，设置self.pretrained_exposures = None



        xyz = np.stack((np.asarray(plydata.elements[0]["x"]),
                        np.asarray(plydata.elements[0]["y"]),
                        np.asarray(plydata.elements[0]["z"])),  axis=1)
        从PLY文件的第一个元素中提取x, y, z坐标，堆叠成形状为(N, 3)的数组
        
        opacities = np.asarray(plydata.elements[0]["opacity"])[..., np.newaxis]提取opacity属性，增加一个新维度使其变为(N, 1)

        features_dc = np.zeros((xyz.shape[0], 3, 1))
        features_dc[:, 0, 0] = np.asarray(plydata.elements[0]["f_dc_0"])
        features_dc[:, 1, 0] = np.asarray(plydata.elements[0]["f_dc_1"])
        features_dc[:, 2, 0] = np.asarray(plydata.elements[0]["f_dc_2"])
        创建形状为(N, 3, 1)的数组

        extra_f_names = [p.name for p in plydata.elements[0].properties if p.name.startswith("f_rest_")]
        extra_f_names = sorted(extra_f_names, key = lambda x: int(x.split('_')[-1]))
        查找所有以"f_rest_"开头的属性名，按后缀数字排序（确保正确顺序）


        assert len(extra_f_names)==3*(self.max_sh_degree + 1) ** 2 - 3 检查高频分量数量是否符合预期，公式是3个通道 × (球谐系数总数 - DC分量)
        
        features_extra = np.zeros((xyz.shape[0], len(extra_f_names)))
        for idx, attr_name in enumerate(extra_f_names):
            features_extra[:, idx] = np.asarray(plydata.elements[0][attr_name])
        加载球谐高频分量，创建空数组存储高频分量，按顺序填充每个属性值
        
        # Reshape (P,F*SH_coeffs) to (P, F, SH_coeffs except DC)
        features_extra = features_extra.reshape((features_extra.shape[0], 3, (self.max_sh_degree + 1) ** 2 - 1))
        从(N, 45)重塑为(N, 3, 15)

        scale_names = [p.name for p in plydata.elements[0].properties if p.name.startswith("scale_")]
        scale_names = sorted(scale_names, key = lambda x: int(x.split('_')[-1]))
        scales = np.zeros((xyz.shape[0], len(scale_names)))
        for idx, attr_name in enumerate(scale_names):
            scales[:, idx] = np.asarray(plydata.elements[0][attr_name])
            识别所有"scale_"属性并按数字排序，创建并填充缩放参数数组(N, 3)



        rot_names = [p.name for p in plydata.elements[0].properties if p.name.startswith("rot")]
        rot_names = sorted(rot_names, key = lambda x: int(x.split('_')[-1]))
        rots = np.zeros((xyz.shape[0], len(rot_names)))
        for idx, attr_name in enumerate(rot_names):
            rots[:, idx] = np.asarray(plydata.elements[0][attr_name])
        与缩放类似
        
        self._xyz = nn.Parameter(torch.tensor(xyz, dtype=torch.float, device="cuda").requires_grad_(True))初始化位置参数，转换为PyTorch张量
        
        self._features_dc = nn.Parameter(torch.tensor(features_dc, dtype=torch.float, device="cuda").transpose(1, 2).contiguous().requires_grad_(True))转换为张量，从(N, 3, 1)到(N, 1, 3)
        
        self._features_rest = nn.Parameter(torch.tensor(features_extra, dtype=torch.float, device="cuda").transpose(1, 2).contiguous().requires_grad_(True))从(N, 3, 15)到(N, 15, 3)
        
        self._opacity = nn.Parameter(torch.tensor(opacities, dtype=torch.float, device="cuda").requires_grad_(True))
        
        self._scaling = nn.Parameter(torch.tensor(scales, dtype=torch.float, device="cuda").requires_grad_(True))
        
        self._rotation = nn.Parameter(torch.tensor(rots, dtype=torch.float, device="cuda").requires_grad_(True))

        self.active_sh_degree = self.max_sh_degree设置激活的球谐阶数，使用最大阶数


def replace_tensor_to_optimizer(self, tensor, name):

        optimizable_tensors = {}
        
        for group in self.optimizer.param_groups:
            if group["name"] == name:找到名称匹配的目标参数组
                stored_state = self.optimizer.state.get(group['params'][0], None)获取当前参数的优化器状态
                stored_state["exp_avg"] = torch.zeros_like(tensor)重置优化器状态
                stored_state["exp_avg_sq"] = torch.zeros_like(tensor)重置优化器状态

            遍历优化器的所有参数组，每个参数组包含一组共享相同超参数的参数


                del self.optimizer.state[group['params'][0]]删除旧参数的优化器状态
                
                group["params"][0] = nn.Parameter(tensor.requires_grad_(True))替换参数，启用梯度计算将参数组的第一个参数替换为新的张量，
                
                self.optimizer.state[group['params'][0]] = stored_state 将重置后的优化器状态关联到新参数

                optimizable_tensors[group["name"]] = group["params"][0]存储更新后的参数，以参数组名称为键，添加到返回字典中
                
        return optimizable_tensors





def _prune_optimizer(self, mask):

        optimizable_tensors = {}
        
        for group in self.optimizer.param_groups:
        
            stored_state = self.optimizer.state.get(group['params'][0], None)获取当前参数的优化器状态，如果不存在则返回None
            
            if stored_state is not None:如果该参数有优化器状态
            
                stored_state["exp_avg"] = stored_state["exp_avg"][mask]对一阶矩估计(动量)应用掩码
                
                stored_state["exp_avg_sq"] = stored_state["exp_avg_sq"][mask]对二阶矩估计(方差)应用掩码

                del self.optimizer.state[group['params'][0]]删除旧参数的优化器状态条目
                
                group["params"][0] = nn.Parameter((group["params"][0][mask].requires_grad_(True)))对当前参数应用掩码然后确保梯度计算，替换参数组中的原参数
                
                self.optimizer.state[group['params'][0]] = stored_state将修剪后的状态关联到新参数

                optimizable_tensors[group["name"]] = group["params"][0]存储修剪后的参数到返回字典
            else:处理没有优化器状态的情况
                group["params"][0] = nn.Parameter(group["params"][0][mask].requires_grad_(True))应用掩码修剪参数，不需要处理优化器状态
                optimizable_tensors[group["name"]] = group["params"][0]
        return optimizable_tensors


